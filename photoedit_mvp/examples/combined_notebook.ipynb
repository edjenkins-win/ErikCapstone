{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e766476971bf62",
   "metadata": {},
   "source": [
    "# AI-Powered Photo Editing: Comprehensive Guide to Intelligent Image Enhancement\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook combines three key aspects of our AI-powered photo editing system:\n",
    "\n",
    "1. **AI-Powered Image Analysis**: Automatically analyzing image content and recommending adjustments\n",
    "2. **Natural Language Photo Editing**: Editing photos using plain English instructions\n",
    "3. **RAG-Based Style Recommendations**: Suggesting cinematic styles based on image content\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Photo editing traditionally requires significant technical knowledge and artistic skill. Users need to understand complex concepts like exposure, contrast, color balance, and composition to achieve professional-looking results. This creates a high barrier to entry for casual photographers who want to enhance their images but lack the technical expertise.\n",
    "\n",
    "Additionally, the editing process can be time-consuming, requiring multiple adjustments and trial-and-error to achieve the desired look. This is especially challenging when dealing with different types of images (landscapes, portraits, low-light scenes) that each require specialized editing approaches.\n",
    "\n",
    "Traditional photo editing interfaces rely on technical sliders, complex terminology, and specialized knowledge that can be intimidating for casual users. Even with simplified consumer apps, users must:\n",
    "\n",
    "1. Understand what each control does (exposure, contrast, saturation, etc.)\n",
    "2. Know which adjustments to make for specific visual goals\n",
    "3. Make multiple trial-and-error attempts to achieve desired results\n",
    "4. Remember which combinations of settings create specific looks\n",
    "\n",
    "Creating professional-looking, cinematic styles for photos traditionally requires:\n",
    "\n",
    "1. **Extensive knowledge of cinematography** and color grading techniques\n",
    "2. **Understanding of visual aesthetics** from different film genres and directors\n",
    "3. **Technical expertise** in complex editing software\n",
    "4. **Time-consuming experimentation** to achieve desired looks\n",
    "\n",
    "## How Generative AI Solves These Problems\n",
    "\n",
    "Generative AI transforms the photo editing experience by bringing intelligence and automation to the process. Instead of requiring users to understand technical details, AI can:\n",
    "\n",
    "1. **Analyze image content** to understand what's in the photo (people, landscapes, objects)\n",
    "2. **Assess technical qualities** like lighting conditions, color balance, and exposure\n",
    "3. **Recommend appropriate adjustments** based on the specific content and conditions\n",
    "4. **Understand natural language instructions** from users who can describe what they want in plain English\n",
    "5. **Suggest cinematic styles** that match the image content using knowledge of cinematography techniques\n",
    "\n",
    "Natural Language Processing (NLP) combined with computer vision creates a revolutionary approach to photo editing. Instead of manipulating technical controls, users can simply describe what they want in plain English:\n",
    "\n",
    "- \"Make the sunset colors more vibrant\"\n",
    "- \"Add more contrast and warmth to the portrait\"\n",
    "- \"Give this landscape a dramatic cinematic look\"\n",
    "- \"Fix the lighting in this dark indoor photo\"\n",
    "\n",
    "Retrieval Augmented Generation (RAG) combined with computer vision creates an intelligent style recommendation system that:\n",
    "\n",
    "1. **Analyzes image content** to understand the scene type, lighting conditions, and subjects\n",
    "2. **Retrieves knowledge** about cinematography techniques and styles from a curated database\n",
    "3. **Matches content to appropriate styles** based on cinematography principles\n",
    "4. **Explains recommendations** with clear reasoning about why each style works for the image\n",
    "5. **Allows natural language queries** so users can describe the look they want to achieve\n",
    "\n",
    "This notebook demonstrates how our AI Photo Editor uses these capabilities to make professional-quality editing accessible to everyone, regardless of technical expertise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab107d73c37ed5",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "import requests\n",
    "import io\n",
    "from typing import Dict, List, Any, Tuple, Optional, Callable, Union, BinaryIO\n",
    "\n",
    "# Set up matplotlib for displaying images\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define supported image formats\n",
    "SUPPORTED_FORMATS = {\n",
    "    # Standard formats\n",
    "    '.jpg': 'JPEG',\n",
    "    '.jpeg': 'JPEG',\n",
    "    '.png': 'PNG',\n",
    "    '.tiff': 'TIFF',\n",
    "    '.tif': 'TIFF',\n",
    "    '.bmp': 'BMP',\n",
    "    '.gif': 'GIF',\n",
    "}\n",
    "\n",
    "# Implementation of utility functions\n",
    "def load_image(source: Union[str, BinaryIO, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Load an image from a file path, URL, file object, or numpy array.\n",
    "\n",
    "    Args:\n",
    "        source: Source image as a file path, URL, file object, or numpy array\n",
    "\n",
    "    Returns:\n",
    "        Loaded image as numpy array in RGB format\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If image could not be loaded\n",
    "    \"\"\"\n",
    "    # If source is already a numpy array, just return it\n",
    "    if isinstance(source, np.ndarray):\n",
    "        return source\n",
    "\n",
    "    # If source is a file path or URL, load from path or download from URL\n",
    "    if isinstance(source, str):\n",
    "        # Check if source is a URL\n",
    "        if source.startswith('http://') or source.startswith('https://'):\n",
    "            try:\n",
    "                # Download the image from the URL\n",
    "                response = requests.get(source, timeout=10)\n",
    "                response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "                # Create a file-like object from the response content\n",
    "                image_data = io.BytesIO(response.content)\n",
    "\n",
    "                # Open the image using PIL\n",
    "                with Image.open(image_data) as img:\n",
    "                    # Convert to RGB if needed\n",
    "                    if img.mode != 'RGB':\n",
    "                        img = img.convert('RGB')\n",
    "                    return np.array(img)\n",
    "            except Exception as url_error:\n",
    "                raise ValueError(f\"Failed to load image from URL {source}: {str(url_error)}\")\n",
    "        else:\n",
    "            # Handle local file path\n",
    "            try:\n",
    "                with Image.open(source) as img:\n",
    "                    # Convert to RGB if needed\n",
    "                    if img.mode != 'RGB':\n",
    "                        img = img.convert('RGB')\n",
    "                    return np.array(img)\n",
    "            except Exception as pil_error:\n",
    "                # Fall back to OpenCV\n",
    "                try:\n",
    "                    image = cv2.imread(source)\n",
    "                    if image is None:\n",
    "                        raise ValueError(f\"Could not load image at {source}\")\n",
    "                    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                except Exception as cv_error:\n",
    "                    raise ValueError(f\"Failed to load image with both PIL and OpenCV: {str(cv_error)}\")\n",
    "\n",
    "    # If source is a file-like object, load from file object\n",
    "    try:\n",
    "        # Save the current position\n",
    "        pos = source.tell()\n",
    "\n",
    "        # Reset to beginning\n",
    "        source.seek(0)\n",
    "\n",
    "        with Image.open(source) as img:\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            image_array = np.array(img)\n",
    "\n",
    "        # Reset to original position\n",
    "        source.seek(pos)\n",
    "        return image_array\n",
    "    except Exception as pil_error:\n",
    "        # Fall back to OpenCV\n",
    "        try:\n",
    "            # Reset to beginning\n",
    "            source.seek(0)\n",
    "            file_bytes = np.asarray(bytearray(source.read()), dtype=np.uint8)\n",
    "            image = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Could not decode image from file object\")\n",
    "            # Reset to original position\n",
    "            source.seek(pos)\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as cv_error:\n",
    "            # Reset position before raising\n",
    "            source.seek(pos)\n",
    "            raise ValueError(f\"Failed to load image with both PIL and OpenCV: {str(cv_error)}\")\n",
    "\n",
    "def save_image(image: np.ndarray, output_path: str, quality: int = 95) -> None:\n",
    "    \"\"\"Save an image to file path.\n",
    "\n",
    "    Args:\n",
    "        image: Image as numpy array in RGB format\n",
    "        output_path: Path where the image will be saved\n",
    "        quality: Quality for lossy formats (0-100)\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If image saving fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Convert numpy array to PIL Image\n",
    "        pil_image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "\n",
    "        # Determine format from file extension\n",
    "        ext = os.path.splitext(output_path)[1].lower()\n",
    "        format_name = SUPPORTED_FORMATS.get(ext, 'JPEG')\n",
    "\n",
    "        # Save with appropriate parameters\n",
    "        save_args = {}\n",
    "        if format_name == 'JPEG':\n",
    "            save_args['quality'] = quality\n",
    "            save_args['optimize'] = True\n",
    "        elif format_name == 'PNG':\n",
    "            save_args['optimize'] = True\n",
    "\n",
    "        pil_image.save(output_path, format=format_name, **save_args)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error saving image to {output_path}: {str(e)}\")\n",
    "\n",
    "def normalize_image(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize image values to [0, 1] range.\"\"\"\n",
    "    return image.astype(np.float32) / 255.0\n",
    "\n",
    "def denormalize_image(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert normalized image back to [0, 255] range.\"\"\"\n",
    "    return (image * 255).astype(np.uint8)\n",
    "\n",
    "# Define Adjustment class\n",
    "class Adjustment:\n",
    "    \"\"\"Represents an image adjustment parameter.\"\"\"\n",
    "\n",
    "    def __init__(self, parameter: str, suggested: float, unit: str = \"\", description: str = \"\"):\n",
    "        \"\"\"Initialize an adjustment.\n",
    "\n",
    "        Args:\n",
    "            parameter: Name of the parameter to adjust\n",
    "            suggested: Suggested value for the adjustment\n",
    "            unit: Unit of measurement (e.g., \"EV\", \"percent\")\n",
    "            description: Human-readable description of the adjustment\n",
    "        \"\"\"\n",
    "        self.parameter = parameter\n",
    "        self.suggested = suggested\n",
    "        self.unit = unit\n",
    "        self.description = description\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"String representation of the adjustment.\"\"\"\n",
    "        return f\"Adjustment({self.parameter}, {self.suggested} {self.unit}, '{self.description}')\"\n",
    "\n",
    "# Implementation of ImageAnalyzer class\n",
    "class ImageAnalyzer:\n",
    "    \"\"\"Analyzes images and recommends adjustments.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the image analyzer.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def analyze(self, image_source: Union[str, np.ndarray]) -> List[Adjustment]:\n",
    "        \"\"\"Analyze an image and recommend adjustments.\n",
    "\n",
    "        Args:\n",
    "            image_source: Path to the image or a numpy array\n",
    "\n",
    "        Returns:\n",
    "            List of recommended adjustments\n",
    "        \"\"\"\n",
    "        # Load the image if it's a file path\n",
    "        image = load_image(image_source)\n",
    "\n",
    "        # Analyze the image\n",
    "        adjustments = []\n",
    "\n",
    "        # Check exposure\n",
    "        exposure_adjustment = self._analyze_exposure(image)\n",
    "        if exposure_adjustment:\n",
    "            adjustments.append(exposure_adjustment)\n",
    "\n",
    "        # Check contrast\n",
    "        contrast_adjustment = self._analyze_contrast(image)\n",
    "        if contrast_adjustment:\n",
    "            adjustments.append(contrast_adjustment)\n",
    "\n",
    "        # Check color balance\n",
    "        color_adjustment = self._analyze_color_balance(image)\n",
    "        if color_adjustment:\n",
    "            adjustments.append(color_adjustment)\n",
    "\n",
    "        # Check sharpness\n",
    "        sharpness_adjustment = self._analyze_sharpness(image)\n",
    "        if sharpness_adjustment:\n",
    "            adjustments.append(sharpness_adjustment)\n",
    "\n",
    "        return adjustments\n",
    "\n",
    "    def _analyze_exposure(self, image: np.ndarray) -> Optional[Adjustment]:\n",
    "        \"\"\"Analyze image exposure and recommend adjustment if needed.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "\n",
    "        Returns:\n",
    "            Exposure adjustment or None\n",
    "        \"\"\"\n",
    "        # Convert to grayscale\n",
    "        if len(image.shape) > 2:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray = image\n",
    "\n",
    "        # Calculate histogram\n",
    "        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "\n",
    "        # Normalize histogram\n",
    "        hist_norm = hist / (image.shape[0] * image.shape[1])\n",
    "\n",
    "        # Calculate cumulative distribution function\n",
    "        cdf = hist_norm.cumsum()\n",
    "\n",
    "        # Check for underexposure (too many dark pixels)\n",
    "        if cdf[64] > 0.5:  # More than 50% of pixels are in the darkest quarter\n",
    "            return Adjustment(\n",
    "                parameter=\"exposure\",\n",
    "                suggested=1.0,\n",
    "                unit=\"EV\",\n",
    "                description=\"Image is underexposed, increase brightness\"\n",
    "            )\n",
    "\n",
    "        # Check for overexposure (too many bright pixels)\n",
    "        if cdf[192] < 0.5:  # Less than 50% of pixels are below the brightest quarter\n",
    "            return Adjustment(\n",
    "                parameter=\"exposure\",\n",
    "                suggested=-0.5,\n",
    "                unit=\"EV\",\n",
    "                description=\"Image is overexposed, decrease brightness\"\n",
    "            )\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _analyze_contrast(self, image: np.ndarray) -> Optional[Adjustment]:\n",
    "        \"\"\"Analyze image contrast and recommend adjustment if needed.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "\n",
    "        Returns:\n",
    "            Contrast adjustment or None\n",
    "        \"\"\"\n",
    "        # Convert to grayscale\n",
    "        if len(image.shape) > 2:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray = image\n",
    "\n",
    "        # Calculate standard deviation as a measure of contrast\n",
    "        std_dev = np.std(gray)\n",
    "\n",
    "        # Low contrast\n",
    "        if std_dev < 30:\n",
    "            return Adjustment(\n",
    "                parameter=\"contrast\",\n",
    "                suggested=1.3,\n",
    "                unit=\"multiplier\",\n",
    "                description=\"Image has low contrast, increase for better definition\"\n",
    "            )\n",
    "\n",
    "        # High contrast\n",
    "        if std_dev > 80:\n",
    "            return Adjustment(\n",
    "                parameter=\"contrast\",\n",
    "                suggested=0.8,\n",
    "                unit=\"multiplier\",\n",
    "                description=\"Image has high contrast, decrease for better balance\"\n",
    "            )\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _analyze_color_balance(self, image: np.ndarray) -> Optional[Adjustment]:\n",
    "        \"\"\"Analyze color balance and recommend adjustment if needed.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "\n",
    "        Returns:\n",
    "            Color balance adjustment or None\n",
    "        \"\"\"\n",
    "        # Skip grayscale images\n",
    "        if len(image.shape) <= 2 or image.shape[2] == 1:\n",
    "            return None\n",
    "\n",
    "        # Calculate average color\n",
    "        avg_color = np.mean(image, axis=(0, 1))\n",
    "\n",
    "        # Check for color cast\n",
    "        r, g, b = avg_color\n",
    "\n",
    "        # Calculate color balance\n",
    "        max_channel = max(r, g, b)\n",
    "        min_channel = min(r, g, b)\n",
    "\n",
    "        # If there's a significant color cast\n",
    "        if max_channel - min_channel > 15:\n",
    "            # Blue cast (too cool)\n",
    "            if b > r and b > g:\n",
    "                return Adjustment(\n",
    "                    parameter=\"temperature\",\n",
    "                    suggested=0.2,\n",
    "                    unit=\"shift\",\n",
    "                    description=\"Image has a cool/blue cast, warm it up\"\n",
    "                )\n",
    "\n",
    "            # Red cast (too warm)\n",
    "            if r > b and r > g:\n",
    "                return Adjustment(\n",
    "                    parameter=\"temperature\",\n",
    "                    suggested=-0.2,\n",
    "                    unit=\"shift\",\n",
    "                    description=\"Image has a warm/red cast, cool it down\"\n",
    "                )\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _analyze_sharpness(self, image: np.ndarray) -> Optional[Adjustment]:\n",
    "        \"\"\"Analyze image sharpness and recommend adjustment if needed.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "\n",
    "        Returns:\n",
    "            Sharpness adjustment or None\n",
    "        \"\"\"\n",
    "        # Convert to grayscale\n",
    "        if len(image.shape) > 2:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray = image\n",
    "\n",
    "        # Calculate Laplacian variance as a measure of sharpness\n",
    "        laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "        sharpness = np.var(laplacian)\n",
    "\n",
    "        # Low sharpness\n",
    "        if sharpness < 100:\n",
    "            return Adjustment(\n",
    "                parameter=\"sharpening\",\n",
    "                suggested=0.5,\n",
    "                unit=\"strength\",\n",
    "                description=\"Image is soft, increase sharpness\"\n",
    "            )\n",
    "\n",
    "        return None\n",
    "\n",
    "# Implementation of ImageExecutor class\n",
    "class ImageExecutor:\n",
    "    \"\"\"Applies adjustments to images.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"Initialize the image executor.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration dictionary\n",
    "        \"\"\"\n",
    "        self.config = config or {}\n",
    "        self._validate_config()\n",
    "\n",
    "    def _validate_config(self) -> None:\n",
    "        \"\"\"Validate and set default configuration parameters.\"\"\"\n",
    "        defaults = {\n",
    "            'max_adjustment_intensity': 1.0,  # Scale factor for all adjustments (0-1)\n",
    "        }\n",
    "\n",
    "        for key, value in defaults.items():\n",
    "            if key not in self.config:\n",
    "                self.config[key] = value\n",
    "\n",
    "    def apply(self, image_source: Union[str, np.ndarray], adjustments: List[Adjustment], style: str = None) -> np.ndarray:\n",
    "        \"\"\"Apply adjustments to an image.\n",
    "\n",
    "        Args:\n",
    "            image_source: Input image as file path or numpy array\n",
    "            adjustments: List of adjustments to apply\n",
    "            style: Optional style preset name\n",
    "\n",
    "        Returns:\n",
    "            Processed image as numpy array\n",
    "        \"\"\"\n",
    "        # Load the image if it's a file path\n",
    "        image = load_image(image_source)\n",
    "        result = image.copy()\n",
    "\n",
    "        # First apply the style preset if specified\n",
    "        if style:\n",
    "            result = self._apply_style(result, style)\n",
    "\n",
    "        # Then apply individual adjustments\n",
    "        for adjustment in adjustments:\n",
    "            result = self._apply_adjustment(result, adjustment)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _apply_style(self, image: np.ndarray, style: str) -> np.ndarray:\n",
    "        \"\"\"Apply a style preset to an image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "            style: Style preset name\n",
    "\n",
    "        Returns:\n",
    "            Processed image\n",
    "        \"\"\"\n",
    "        # Copy the image to avoid modifying the original\n",
    "        result = image.copy()\n",
    "\n",
    "        # Apply the appropriate style preset\n",
    "        style_lower = style.lower().replace(' ', '-')\n",
    "\n",
    "        if style_lower == \"auto-enhance\":\n",
    "            result = self._style_auto_enhance(result)\n",
    "        elif style_lower == \"portrait\":\n",
    "            result = self._style_portrait(result)\n",
    "        elif style_lower == \"vintage\":\n",
    "            result = self._style_vintage(result)\n",
    "        # Add new cinematic styles\n",
    "        elif style_lower == \"cinematic-teal-orange\" or style_lower == \"cinematic-teal-&-orange\":\n",
    "            result = self._style_cinematic_teal_orange(result)\n",
    "        elif style_lower == \"film-noir\":\n",
    "            result = self._style_film_noir(result)\n",
    "        elif style_lower == \"anamorphic\":\n",
    "            result = self._style_anamorphic(result)\n",
    "        elif style_lower == \"blockbuster\":\n",
    "            result = self._style_blockbuster(result)\n",
    "        elif style_lower == \"dreamy\":\n",
    "            result = self._style_dreamy(result)\n",
    "        else:\n",
    "            logger.warning(f\"Unknown style preset: {style}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _style_auto_enhance(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply the Auto-Enhance style.\"\"\"\n",
    "        # Convert to LAB color space\n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "\n",
    "        # Split channels\n",
    "        l, a, b = cv2.split(lab)\n",
    "\n",
    "        # Apply CLAHE to L channel\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        l = clahe.apply(l)\n",
    "\n",
    "        # Merge channels\n",
    "        lab = cv2.merge([l, a, b])\n",
    "\n",
    "        # Convert back to RGB\n",
    "        enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "        # Apply slight sharpening\n",
    "        enhanced = self._apply_sharpening(enhanced, 0.2)\n",
    "\n",
    "        return enhanced\n",
    "\n",
    "    def _style_portrait(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply the Portrait style.\"\"\"\n",
    "        # Start with auto-enhance\n",
    "        result = self._style_auto_enhance(image)\n",
    "\n",
    "        # Apply skin smoothing (bilateral filter)\n",
    "        # Convert to float32 for better precision\n",
    "        float_img = result.astype(np.float32) / 255.0\n",
    "        # Apply bilateral filter for edge-preserving smoothing\n",
    "        smoothed = cv2.bilateralFilter(float_img, 9, 75, 75)\n",
    "        # Convert back to uint8\n",
    "        smoothed = (smoothed * 255).astype(np.uint8)\n",
    "\n",
    "        # Warm up the image slightly\n",
    "        result = self._apply_temperature(smoothed, 0.1)\n",
    "\n",
    "        # Slightly increase saturation\n",
    "        result = self._apply_saturation(result, 0.1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _style_vintage(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply the Vintage style.\"\"\"\n",
    "        # Create a vintage look\n",
    "        # Slightly reduce contrast\n",
    "        result = self._apply_contrast(image, 0.8)\n",
    "\n",
    "        # Apply a slight sepia tone\n",
    "        sepia = np.array([\n",
    "            [0.393, 0.769, 0.189],\n",
    "            [0.349, 0.686, 0.168],\n",
    "            [0.272, 0.534, 0.131]\n",
    "        ])\n",
    "\n",
    "        # Convert to float for matrix multiplication\n",
    "        float_img = result.astype(np.float32) / 255.0\n",
    "        sepia_img = np.zeros_like(float_img)\n",
    "\n",
    "        # Apply sepia matrix\n",
    "        for i in range(3):\n",
    "            sepia_img[:, :, i] = np.sum(float_img * sepia[i], axis=2)\n",
    "\n",
    "        # Clip values to valid range\n",
    "        sepia_img = np.clip(sepia_img, 0, 1.0)\n",
    "\n",
    "        # Convert back to uint8\n",
    "        result = (sepia_img * 255).astype(np.uint8)\n",
    "\n",
    "        # Add slight vignette\n",
    "        height, width = result.shape[:2]\n",
    "\n",
    "        # Create vignette mask\n",
    "        x = np.linspace(-1, 1, width)\n",
    "        y = np.linspace(-1, 1, height)\n",
    "        x, y = np.meshgrid(x, y)\n",
    "        radius = np.sqrt(x**2 + y**2)\n",
    "\n",
    "        # Create vignette\n",
    "        vignette = np.clip(1.0 - radius * 0.5, 0, 1.0)\n",
    "        vignette = np.dstack([vignette] * 3)\n",
    "\n",
    "        # Apply vignette\n",
    "        result = (result.astype(np.float32) * vignette).astype(np.uint8)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _style_cinematic_teal_orange(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply the Cinematic Teal & Orange style.\"\"\"\n",
    "        # Enhance contrast first\n",
    "        result = self._apply_contrast(image, 1.3)\n",
    "\n",
    "        # Convert to float for processing\n",
    "        img_float = result.astype(np.float32) / 255.0\n",
    "\n",
    "        # Split into channels\n",
    "        b, g, r = cv2.split(img_float)\n",
    "\n",
    "        # Manipulate shadows (blue/teal) and highlights (orange)\n",
    "        # Boost blue in shadows\n",
    "        shadows = 1.0 - ((r + g) / 2.0)  # Approximate shadow areas\n",
    "        b_new = b + (shadows * 0.15)  # Add blue to shadows\n",
    "\n",
    "        # Boost orange in highlights\n",
    "        highlights = ((r + g) / 2.0)  # Approximate highlight areas\n",
    "        r_new = r + (highlights * 0.1)  # Add red to highlights\n",
    "        g_new = g + (highlights * 0.05)  # Add a bit of green for orange\n",
    "\n",
    "        # Clip to valid range\n",
    "        b_new = np.clip(b_new, 0, 1.0)\n",
    "        g_new = np.clip(g_new, 0, 1.0)\n",
    "        r_new = np.clip(r_new, 0, 1.0)\n",
    "\n",
    "        # Merge channels\n",
    "        result = cv2.merge([b_new, g_new, r_new])\n",
    "\n",
    "        # Convert back to uint8\n",
    "        result = (result * 255).astype(np.uint8)\n",
    "\n",
    "        # Apply slight sharpening\n",
    "        result = self._apply_sharpening(result, 0.25)\n",
    "\n",
    "        # Add subtle vignette\n",
    "        height, width = result.shape[:2]\n",
    "        x = np.linspace(-1, 1, width)\n",
    "        y = np.linspace(-1, 1, height)\n",
    "        x, y = np.meshgrid(x, y)\n",
    "        radius = np.sqrt(x**2 + y**2)\n",
    "        vignette = np.clip(1.0 - radius * 0.3, 0, 1.0)\n",
    "        vignette = np.dstack([vignette] * 3)\n",
    "        result = (result.astype(np.float32) * vignette).astype(np.uint8)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _style_film_noir(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply the Film Noir style.\"\"\"\n",
    "        # First convert to black and white with high contrast\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Apply CLAHE for better contrast distribution\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "        gray = clahe.apply(gray)\n",
    "\n",
    "        # Enhance contrast\n",
    "        gray = cv2.convertScaleAbs(gray, alpha=1.5, beta=-10)\n",
    "\n",
    "        # Convert back to RGB\n",
    "        result = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # Add strong vignette for dramatic effect\n",
    "        height, width = result.shape[:2]\n",
    "        x = np.linspace(-1, 1, width)\n",
    "        y = np.linspace(-1, 1, height)\n",
    "        x, y = np.meshgrid(x, y)\n",
    "        radius = np.sqrt(x**2 + y**2)\n",
    "        vignette = np.clip(1.0 - radius * 0.8, 0, 1.0)\n",
    "        vignette = np.dstack([vignette] * 3)\n",
    "        result = (result.astype(np.float32) * vignette).astype(np.uint8)\n",
    "\n",
    "        # Add film grain\n",
    "        noise = np.random.normal(0, 0.03, result.shape).astype(np.float32)\n",
    "        result = np.clip(result.astype(np.float32) / 255.0 + noise, 0, 1) * 255\n",
    "        result = result.astype(np.uint8)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _style_anamorphic(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply the Anamorphic style.\"\"\"\n",
    "        # Enhance contrast and color\n",
    "        result = self._apply_contrast(image, 1.25)\n",
    "        result = self._apply_saturation(result, 0.1)\n",
    "\n",
    "        # Shift color balance slightly toward blue\n",
    "        result = self._apply_temperature(result, -0.1)\n",
    "\n",
    "        # Add horizontal lens flare effect (blue streak)\n",
    "        height, width = result.shape[:2]\n",
    "\n",
    "        # Create horizontal flare in random position\n",
    "        flare_y = np.random.randint(height // 4, 3 * height // 4)\n",
    "        flare = np.zeros_like(result, dtype=np.float32)\n",
    "\n",
    "        # Create horizontal blue streak\n",
    "        for y in range(max(0, flare_y - 5), min(height, flare_y + 6)):\n",
    "            intensity = 1.0 - abs(y - flare_y) / 5.0\n",
    "            for x in range(width):\n",
    "                # Blue-tinted flare with falloff from center\n",
    "                dist_from_center = abs(x - width // 2) / (width // 2)\n",
    "                flare_intensity = intensity * (1.0 - dist_from_center**2) * 0.4\n",
    "                flare[y, x, 0] = flare_intensity * 0.8  # Blue channel\n",
    "                flare[y, x, 1] = flare_intensity * 0.3  # Green channel\n",
    "                flare[y, x, 2] = flare_intensity * 0.2  # Red channel\n",
    "\n",
    "        # Add flare to image\n",
    "        result = np.clip(result.astype(np.float32) + flare * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Add letterbox effect to simulate widescreen\n",
    "        letterbox_height = height // 6\n",
    "        result[0:letterbox_height, :] = [0, 0, 0]\n",
    "        result[height - letterbox_height:height, :] = [0, 0, 0]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _style_blockbuster(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply the Blockbuster style.\"\"\"\n",
    "        # Increase exposure slightly\n",
    "        result = self._apply_exposure(image, 0.1)\n",
    "\n",
    "        # Enhance contrast dramatically\n",
    "        result = self._apply_contrast(result, 1.4)\n",
    "\n",
    "        # Boost saturation for vivid colors\n",
    "        result = self._apply_saturation(result, 0.25)\n",
    "\n",
    "        # Enhance sharpness\n",
    "        result = self._apply_sharpening(result, 0.3)\n",
    "\n",
    "        # Add color tint based on dominant colors (typical blockbuster color grading)\n",
    "        # Convert to HSV for easier color manipulation\n",
    "        hsv = cv2.cvtColor(result, cv2.COLOR_RGB2HSV)\n",
    "        h, s, v = cv2.split(hsv)\n",
    "\n",
    "        # Shift colors slightly toward blue/cyan for shadows and orange/yellow for highlights\n",
    "        # This creates the modern blockbuster look\n",
    "        mask = v < 128  # Shadow areas\n",
    "        h[mask] = np.clip(h[mask] + 10, 0, 179)  # Shift toward blue-cyan\n",
    "\n",
    "        mask = v >= 128  # Highlight areas\n",
    "        h[mask] = np.clip(h[mask] - 10, 0, 179)  # Shift toward orange-yellow\n",
    "\n",
    "        # Merge channels\n",
    "        hsv = cv2.merge([h, s, v])\n",
    "\n",
    "        # Convert back to RGB\n",
    "        result = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "        # Add subtle vignette\n",
    "        height, width = result.shape[:2]\n",
    "        x = np.linspace(-1, 1, width)\n",
    "        y = np.linspace(-1, 1, height)\n",
    "        x, y = np.meshgrid(x, y)\n",
    "        radius = np.sqrt(x**2 + y**2)\n",
    "        vignette = np.clip(1.0 - radius * 0.2, 0, 1.0)\n",
    "        vignette = np.dstack([vignette] * 3)\n",
    "        result = (result.astype(np.float32) * vignette).astype(np.uint8)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _style_dreamy(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply the Dreamy style.\"\"\"\n",
    "        # Reduce contrast\n",
    "        result = self._apply_contrast(image, 0.85)\n",
    "\n",
    "        # Add warmth\n",
    "        result = self._apply_temperature(result, 0.15)\n",
    "\n",
    "        # Apply soft glow effect\n",
    "        # Create blurred version\n",
    "        blur = cv2.GaussianBlur(result, (21, 21), 0)\n",
    "\n",
    "        # Blend with original (soft glow effect)\n",
    "        result = cv2.addWeighted(result, 0.7, blur, 0.3, 0)\n",
    "\n",
    "        # Apply noise reduction\n",
    "        result = self._apply_noise_reduction(result, 0.4)\n",
    "\n",
    "        # Add dreamy haze/fog effect\n",
    "        height, width = result.shape[:2]\n",
    "        fog = np.ones_like(result) * 255  # White fog\n",
    "\n",
    "        # Create fog gradient\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                # Calculate distance from top left\n",
    "                dist = np.sqrt((x / width)**2 + (y / height)**2)\n",
    "                fog_alpha = 0.15 * (1.0 - dist)  # Stronger fog in top-left corner\n",
    "                result[y, x] = cv2.addWeighted(result[y, x], 1 - fog_alpha, fog[y, x], fog_alpha, 0)\n",
    "\n",
    "        # Add vignette for dreamy border effect\n",
    "        x = np.linspace(-1, 1, width)\n",
    "        y = np.linspace(-1, 1, height)\n",
    "        x, y = np.meshgrid(x, y)\n",
    "        radius = np.sqrt(x**2 + y**2)\n",
    "        vignette = np.clip(1.0 - radius * 0.4, 0, 1.0)\n",
    "        vignette = np.dstack([vignette] * 3)\n",
    "        result = (result.astype(np.float32) * vignette).astype(np.uint8)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _apply_adjustment(self, image: np.ndarray, adjustment: Adjustment) -> np.ndarray:\n",
    "        \"\"\"Apply a single adjustment to an image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "            adjustment: Adjustment to apply\n",
    "\n",
    "        Returns:\n",
    "            Processed image\n",
    "        \"\"\"\n",
    "        # Scale the adjustment by the max intensity\n",
    "        intensity = self.config['max_adjustment_intensity']\n",
    "\n",
    "        # Apply the appropriate adjustment based on parameter\n",
    "        if adjustment.parameter == \"exposure\":\n",
    "            return self._apply_exposure(image, adjustment.suggested * intensity)\n",
    "        elif adjustment.parameter == \"contrast\":\n",
    "            return self._apply_contrast(image, adjustment.suggested)\n",
    "        elif adjustment.parameter == \"noise_reduction\":\n",
    "            return self._apply_noise_reduction(image, adjustment.suggested * intensity)\n",
    "        elif adjustment.parameter == \"sharpening\":\n",
    "            return self._apply_sharpening(image, adjustment.suggested * intensity)\n",
    "        elif adjustment.parameter == \"saturation\":\n",
    "            return self._apply_saturation(image, adjustment.suggested * intensity)\n",
    "        elif adjustment.parameter == \"temperature\":\n",
    "            return self._apply_temperature(image, adjustment.suggested * intensity)\n",
    "        else:\n",
    "            logger.warning(f\"Unknown adjustment parameter: {adjustment.parameter}\")\n",
    "            return image\n",
    "\n",
    "    def _apply_exposure(self, image: np.ndarray, ev: float) -> np.ndarray:\n",
    "        \"\"\"Apply exposure adjustment.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "            ev: Exposure value adjustment in EV\n",
    "\n",
    "        Returns:\n",
    "            Processed image\n",
    "        \"\"\"\n",
    "        # Convert to float\n",
    "        float_img = image.astype(np.float32) / 255.0\n",
    "\n",
    "        # Calculate multiplier from EV\n",
    "        multiplier = 2 ** ev\n",
    "\n",
    "        # Apply adjustment\n",
    "        result = float_img * multiplier\n",
    "\n",
    "        # Clip values\n",
    "        result = np.clip(result, 0, 1.0)\n",
    "\n",
    "        # Convert back to uint8\n",
    "        return (result * 255).astype(np.uint8)\n",
    "\n",
    "    def _apply_contrast(self, image: np.ndarray, multiplier: float) -> np.ndarray:\n",
    "        \"\"\"Apply contrast adjustment.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "            multiplier: Contrast multiplier\n",
    "\n",
    "        Returns:\n",
    "            Processed image\n",
    "        \"\"\"\n",
    "        # Convert to float\n",
    "        float_img = image.astype(np.float32) / 255.0\n",
    "\n",
    "        # Calculate mean\n",
    "        mean = np.mean(float_img, axis=(0, 1), keepdims=True)\n",
    "\n",
    "        # Apply contrast adjustment (center around mean)\n",
    "        result = (float_img - mean) * multiplier + mean\n",
    "\n",
    "        # Clip values\n",
    "        result = np.clip(result, 0, 1.0)\n",
    "\n",
    "        # Convert back to uint8\n",
    "        return (result * 255).astype(np.uint8)\n",
    "\n",
    "    def _apply_noise_reduction(self, image: np.ndarray, strength: float) -> np.ndarray:\n",
    "        \"\"\"Apply noise reduction.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "            strength: Strength of noise reduction (0-1)\n",
    "\n",
    "        Returns:\n",
    "            Processed image\n",
    "        \"\"\"\n",
    "        # Apply bilateral filter\n",
    "        # Scale strength to appropriate range for bilateral filter\n",
    "        d = int(3 + strength * 7)  # Diameter of filter, 3-10\n",
    "        sigma_color = 10 + strength * 90  # 10-100\n",
    "        sigma_space = 10 + strength * 90  # 10-100\n",
    "\n",
    "        return cv2.bilateralFilter(image, d, sigma_color, sigma_space)\n",
    "\n",
    "    def _apply_sharpening(self, image: np.ndarray, strength: float) -> np.ndarray:\n",
    "        \"\"\"Apply sharpening.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "            strength: Strength of sharpening (0-1)\n",
    "\n",
    "        Returns:\n",
    "            Processed image\n",
    "        \"\"\"\n",
    "        # Apply unsharp mask\n",
    "        blurred = cv2.GaussianBlur(image, (0, 0), 3)\n",
    "        sharpened = cv2.addWeighted(\n",
    "            image, 1.0 + strength, \n",
    "            blurred, -strength, \n",
    "            0\n",
    "        )\n",
    "\n",
    "        return sharpened\n",
    "\n",
    "    def _apply_saturation(self, image: np.ndarray, adjustment: float) -> np.ndarray:\n",
    "        \"\"\"Apply saturation adjustment.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "            adjustment: Saturation adjustment (-1 to 1)\n",
    "\n",
    "        Returns:\n",
    "            Processed image\n",
    "        \"\"\"\n",
    "        # Convert to HSV\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "\n",
    "        # Calculate multiplier (1.0 means no change)\n",
    "        if adjustment > 0:\n",
    "            # Increase saturation\n",
    "            multiplier = 1.0 + adjustment\n",
    "        else:\n",
    "            # Decrease saturation\n",
    "            multiplier = 1.0 + adjustment\n",
    "\n",
    "        # Apply multiplier to saturation channel\n",
    "        hsv[:, :, 1] = np.clip(hsv[:, :, 1] * multiplier, 0, 255)\n",
    "\n",
    "        # Convert back to BGR\n",
    "        return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "\n",
    "    def _apply_temperature(self, image: np.ndarray, adjustment: float) -> np.ndarray:\n",
    "        \"\"\"Apply color temperature adjustment.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "            adjustment: Temperature adjustment (-1 to 1)\n",
    "\n",
    "        Returns:\n",
    "            Processed image\n",
    "        \"\"\"\n",
    "        # Copy the image\n",
    "        result = image.copy().astype(np.float32)\n",
    "\n",
    "        # Apply temperature adjustment\n",
    "        if adjustment > 0:\n",
    "            # Warm up (increase red, decrease blue)\n",
    "            result[:, :, 0] = np.clip(result[:, :, 0] * (1 + adjustment * 0.2), 0, 255)  # Red\n",
    "            result[:, :, 2] = np.clip(result[:, :, 2] * (1 - adjustment * 0.1), 0, 255)  # Blue\n",
    "        else:\n",
    "            # Cool down (decrease red, increase blue)\n",
    "            adjustment = abs(adjustment)\n",
    "            result[:, :, 0] = np.clip(result[:, :, 0] * (1 - adjustment * 0.1), 0, 255)  # Red\n",
    "            result[:, :, 2] = np.clip(result[:, :, 2] * (1 + adjustment * 0.2), 0, 255)  # Blue\n",
    "\n",
    "        return result.astype(np.uint8)\n",
    "\n",
    "# Implementation of analyze_image and apply_adjustments functions\n",
    "def analyze_image(image_source):\n",
    "    \"\"\"Analyze an image and return recommended adjustments.\n",
    "\n",
    "    Args:\n",
    "        image_source: Path to the image or a numpy array\n",
    "\n",
    "    Returns:\n",
    "        List of recommended adjustments\n",
    "    \"\"\"\n",
    "    analyzer = ImageAnalyzer()\n",
    "    return analyzer.analyze(image_source)\n",
    "\n",
    "def apply_adjustments(image_source, adjustments, style=None):\n",
    "    \"\"\"Apply adjustments and/or a style to an image.\n",
    "\n",
    "    Args:\n",
    "        image_source: Path to the image or a numpy array\n",
    "        adjustments: List of adjustments to apply\n",
    "        style: Optional style preset name\n",
    "\n",
    "    Returns:\n",
    "        Processed image as a numpy array\n",
    "    \"\"\"\n",
    "    executor = ImageExecutor()\n",
    "    return executor.apply(image_source, adjustments, style)\n",
    "\n",
    "# Implementation of AIImageAnalyzer class\n",
    "class AIImageAnalyzer:\n",
    "    \"\"\"Uses AI models to analyze image content and make intelligent suggestions.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"Initialize the AI image analyzer.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration dictionary for model paths and parameters\n",
    "        \"\"\"\n",
    "        self.config = config or {}\n",
    "        self._validate_config()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Initialize models lazily to reduce startup time\n",
    "        self.scene_model = None\n",
    "        self.face_model = None\n",
    "        self.object_model = None\n",
    "\n",
    "    def _validate_config(self) -> None:\n",
    "        \"\"\"Validate and set default configuration parameters.\"\"\"\n",
    "        defaults = {\n",
    "            'model_path': 'default_model',  # Use a default model in contained environment\n",
    "            'detection_threshold': 0.5,\n",
    "            'max_objects': 10,\n",
    "        }\n",
    "\n",
    "        for key, value in defaults.items():\n",
    "            if key not in self.config:\n",
    "                self.config[key] = value\n",
    "\n",
    "    def _load_scene_model(self):\n",
    "        \"\"\"Load the scene classification model.\"\"\"\n",
    "        try:\n",
    "            # In a real implementation, we would use a model like ResNet pre-trained on Places365\n",
    "            # For demo purposes, we'll simulate loading a model\n",
    "            logger.info(\"Loading scene classification model\")\n",
    "            self.scene_model = True  # Simulate successful loading\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load scene model: {e}\")\n",
    "            self.scene_model = None\n",
    "\n",
    "    def _load_face_model(self):\n",
    "        \"\"\"Load the face detection model.\"\"\"\n",
    "        try:\n",
    "            # In a real implementation, load a face detection model like MTCNN\n",
    "            # For now, we'll use OpenCV's built-in face detector\n",
    "            logger.info(\"Loading face detection model\")\n",
    "            self.face_model = cv2.CascadeClassifier(\n",
    "                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load face model: {e}\")\n",
    "            self.face_model = None\n",
    "\n",
    "    def _load_object_model(self):\n",
    "        \"\"\"Load the object detection model.\"\"\"\n",
    "        try:\n",
    "            # In a real implementation, use a model like YOLO or Faster R-CNN\n",
    "            # For now, we'll simulate loading a model\n",
    "            logger.info(\"Loading object detection model\")\n",
    "            self.object_model = True  # Simulate successful loading\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load object model: {e}\")\n",
    "            self.object_model = None\n",
    "\n",
    "    def analyze(self, image: np.ndarray) -> Tuple[List[Adjustment], Dict[str, Any]]:\n",
    "        \"\"\"Analyze image content and suggest appropriate adjustments.\n",
    "\n",
    "        Args:\n",
    "            image: Input image as numpy array\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing (adjustments list, analysis metadata)\n",
    "        \"\"\"\n",
    "        # Ensure models are loaded\n",
    "        if self.scene_model is None:\n",
    "            self._load_scene_model()\n",
    "        if self.face_model is None:\n",
    "            self._load_face_model()\n",
    "        if self.object_model is None:\n",
    "            self._load_object_model()\n",
    "\n",
    "        # Initialize analysis results\n",
    "        analysis = {\n",
    "            'scene_type': None,\n",
    "            'has_faces': False,\n",
    "            'face_count': 0,\n",
    "            'objects': [],\n",
    "            'lighting_condition': None,\n",
    "            'color_palette': None,\n",
    "        }\n",
    "\n",
    "        # Analyze scene type\n",
    "        scene_type = self._analyze_scene(image)\n",
    "        analysis['scene_type'] = scene_type\n",
    "\n",
    "        # Detect faces\n",
    "        faces = self._detect_faces(image)\n",
    "        analysis['has_faces'] = len(faces) > 0\n",
    "        analysis['face_count'] = len(faces)\n",
    "\n",
    "        # Detect objects\n",
    "        objects = self._detect_objects(image)\n",
    "        analysis['objects'] = objects\n",
    "\n",
    "        # Analyze lighting conditions\n",
    "        lighting = self._analyze_lighting(image)\n",
    "        analysis['lighting_condition'] = lighting\n",
    "\n",
    "        # Extract color palette\n",
    "        color_palette = self._extract_color_palette(image)\n",
    "        analysis['color_palette'] = color_palette\n",
    "\n",
    "        # Generate intelligent adjustment recommendations\n",
    "        adjustments = self._generate_adjustments(image, analysis)\n",
    "\n",
    "        return adjustments, analysis\n",
    "\n",
    "    def _analyze_scene(self, image: np.ndarray) -> str:\n",
    "        \"\"\"Classify the scene type in the image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "\n",
    "        Returns:\n",
    "            Scene type as string\n",
    "        \"\"\"\n",
    "        # In a real implementation, use the scene model to classify the image\n",
    "        # For now, use simple heuristics for demonstration\n",
    "\n",
    "        # Convert to RGB if not already\n",
    "        if len(image.shape) == 2 or image.shape[2] == 1:\n",
    "            # Grayscale image\n",
    "            return \"unknown\"\n",
    "\n",
    "        # Simple color-based heuristics\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "        h, s, v = cv2.split(hsv)\n",
    "\n",
    "        # Check for landscapes based on color distribution\n",
    "        blue_sky = np.mean(h[v > 200]) > 100 and np.mean(h[v > 200]) < 140\n",
    "        green_dominant = np.mean(h) > 35 and np.mean(h) < 85 and np.mean(s) > 50\n",
    "\n",
    "        if blue_sky and green_dominant:\n",
    "            return \"landscape\"\n",
    "        elif blue_sky:\n",
    "            return \"sky\"\n",
    "        elif green_dominant:\n",
    "            return \"nature\"\n",
    "\n",
    "        # Check for indoor/urban scenes\n",
    "        if np.mean(v) < 100:\n",
    "            return \"indoor\"\n",
    "\n",
    "        # Default fallback\n",
    "        return \"general\"\n",
    "\n",
    "    def _detect_faces(self, image: np.ndarray) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Detect faces in the image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "\n",
    "        Returns:\n",
    "            List of face detection results\n",
    "        \"\"\"\n",
    "        if self.face_model is None:\n",
    "            return []\n",
    "\n",
    "        # Convert to grayscale for face detection\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Detect faces\n",
    "        faces = self.face_model.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30)\n",
    "        )\n",
    "\n",
    "        # Format results\n",
    "        face_results = []\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_results.append({\n",
    "                'bbox': (x, y, x+w, y+h),\n",
    "                'confidence': 0.9,  # Placeholder confidence\n",
    "            })\n",
    "\n",
    "        return face_results\n",
    "\n",
    "    def _detect_objects(self, image: np.ndarray) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Detect objects in the image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "\n",
    "        Returns:\n",
    "            List of object detection results\n",
    "        \"\"\"\n",
    "        # For demonstration, return simulated objects based on scene type\n",
    "        scene_type = self._analyze_scene(image)\n",
    "\n",
    "        if scene_type == \"landscape\":\n",
    "            return [\n",
    "                {'class': 'mountain', 'confidence': 0.8},\n",
    "                {'class': 'tree', 'confidence': 0.7},\n",
    "                {'class': 'sky', 'confidence': 0.9}\n",
    "            ]\n",
    "        elif scene_type == \"indoor\":\n",
    "            return [\n",
    "                {'class': 'chair', 'confidence': 0.6},\n",
    "                {'class': 'table', 'confidence': 0.5}\n",
    "            ]\n",
    "\n",
    "        # Default fallback\n",
    "        return []\n",
    "\n",
    "    def _analyze_lighting(self, image: np.ndarray) -> str:\n",
    "        \"\"\"Analyze lighting conditions in the image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "\n",
    "        Returns:\n",
    "            Lighting condition as string\n",
    "        \"\"\"\n",
    "        # Convert to grayscale\n",
    "        if len(image.shape) > 2:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray = image\n",
    "\n",
    "        # Calculate histogram\n",
    "        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "\n",
    "        # Analyze histogram for lighting conditions\n",
    "        dark_pixels = np.sum(hist[:64])\n",
    "        mid_pixels = np.sum(hist[64:192])\n",
    "        bright_pixels = np.sum(hist[192:])\n",
    "\n",
    "        total_pixels = dark_pixels + mid_pixels + bright_pixels\n",
    "\n",
    "        dark_ratio = dark_pixels / total_pixels\n",
    "        bright_ratio = bright_pixels / total_pixels\n",
    "\n",
    "        if dark_ratio > 0.5:\n",
    "            return \"low_light\"\n",
    "        elif bright_ratio > 0.5:\n",
    "            return \"bright\"\n",
    "        else:\n",
    "            return \"normal\"\n",
    "\n",
    "    def _extract_color_palette(self, image: np.ndarray) -> List[List[int]]:\n",
    "        \"\"\"Extract dominant color palette from the image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "\n",
    "        Returns:\n",
    "            List of RGB color values\n",
    "        \"\"\"\n",
    "        # Resize image for faster processing\n",
    "        small = cv2.resize(image, (100, 100))\n",
    "\n",
    "        # Reshape for k-means\n",
    "        pixels = small.reshape(-1, 3)\n",
    "        pixels = np.float32(pixels)\n",
    "\n",
    "        # Define criteria and apply k-means\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, 0.1)\n",
    "        k = 5  # Number of colors to extract\n",
    "        _, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "        # Convert centers to integer RGB values\n",
    "        centers = np.uint8(centers)\n",
    "\n",
    "        # Count pixels in each cluster\n",
    "        counts = np.bincount(labels.flatten())\n",
    "\n",
    "        # Sort colors by frequency\n",
    "        sorted_indices = np.argsort(counts)[::-1]\n",
    "        palette = [centers[i].tolist() for i in sorted_indices]\n",
    "\n",
    "        return palette\n",
    "\n",
    "    def _generate_adjustments(self, image: np.ndarray, analysis: Dict[str, Any]) -> List[Adjustment]:\n",
    "        \"\"\"Generate adjustment recommendations based on image analysis.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "            analysis: Analysis results\n",
    "\n",
    "        Returns:\n",
    "            List of recommended adjustments\n",
    "        \"\"\"\n",
    "        adjustments = []\n",
    "\n",
    "        # Generate scene-specific adjustments\n",
    "        scene_type = analysis.get('scene_type', 'general')\n",
    "        if scene_type == \"landscape\":\n",
    "            # Enhance blues for sky and greens for vegetation\n",
    "            adjustments.append(Adjustment(\n",
    "                parameter=\"saturation\",\n",
    "                suggested=0.2,\n",
    "                unit=\"increase\",\n",
    "                description=\"Enhance landscape colors\"\n",
    "            ))\n",
    "            adjustments.append(Adjustment(\n",
    "                parameter=\"contrast\",\n",
    "                suggested=1.15,\n",
    "                unit=\"multiplier\",\n",
    "                description=\"Boost landscape contrast\"\n",
    "            ))\n",
    "        elif scene_type == \"indoor\":\n",
    "            # Indoor scenes often need white balance correction\n",
    "            adjustments.append(Adjustment(\n",
    "                parameter=\"temperature\",\n",
    "                suggested=0.1,\n",
    "                unit=\"shift\",\n",
    "                description=\"Correct indoor lighting\"\n",
    "            ))\n",
    "\n",
    "        # Lighting-specific adjustments\n",
    "        lighting = analysis.get('lighting_condition', 'normal')\n",
    "        if lighting == \"low_light\":\n",
    "            # Brighten dark images\n",
    "            adjustments.append(Adjustment(\n",
    "                parameter=\"exposure\",\n",
    "                suggested=0.5,\n",
    "                unit=\"EV\",\n",
    "                description=\"Brighten dark image\"\n",
    "            ))\n",
    "            # Reduce noise in low light\n",
    "            adjustments.append(Adjustment(\n",
    "                parameter=\"noise_reduction\",\n",
    "                suggested=0.4,\n",
    "                unit=\"strength\",\n",
    "                description=\"Reduce low-light noise\"\n",
    "            ))\n",
    "        elif lighting == \"bright\":\n",
    "            # Recover highlights in bright images\n",
    "            adjustments.append(Adjustment(\n",
    "                parameter=\"exposure\",\n",
    "                suggested=-0.2,\n",
    "                unit=\"EV\",\n",
    "                description=\"Recover bright highlights\"\n",
    "            ))\n",
    "\n",
    "        # Portrait-specific adjustments\n",
    "        if analysis.get('has_faces', False):\n",
    "            # Enhance portraits\n",
    "            adjustments.append(Adjustment(\n",
    "                parameter=\"temperature\",\n",
    "                suggested=0.1,\n",
    "                unit=\"shift\",\n",
    "                description=\"Warm skin tones\"\n",
    "            ))\n",
    "            # Subtle skin smoothing\n",
    "            adjustments.append(Adjustment(\n",
    "                parameter=\"noise_reduction\",\n",
    "                suggested=0.3,\n",
    "                unit=\"strength\",\n",
    "                description=\"Smooth skin details\"\n",
    "            ))\n",
    "\n",
    "        return adjustments\n",
    "\n",
    "    def suggest_style(self, image: np.ndarray, analysis: Optional[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"Suggest an appropriate style preset based on image content.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "            analysis: Optional pre-computed analysis results\n",
    "\n",
    "        Returns:\n",
    "            Name of the recommended style preset\n",
    "        \"\"\"\n",
    "        # Run analysis if not provided\n",
    "        if analysis is None:\n",
    "            _, analysis = self.analyze(image)\n",
    "\n",
    "        # Suggest style based on content\n",
    "        scene_type = analysis.get('scene_type', 'general')\n",
    "        has_faces = analysis.get('has_faces', False)\n",
    "        lighting = analysis.get('lighting_condition', 'normal')\n",
    "\n",
    "        # Style selection logic\n",
    "        if has_faces:\n",
    "            if lighting == \"low_light\":\n",
    "                return \"Film Noir\"  # Dramatic portrait style\n",
    "            else:\n",
    "                return \"Portrait\"  # Standard portrait style\n",
    "        elif scene_type in [\"landscape\", \"nature\"]:\n",
    "            return \"Cinematic Teal & Orange\"  # Good for landscapes\n",
    "        elif scene_type == \"indoor\" and lighting == \"low_light\":\n",
    "            return \"Anamorphic\"  # Good for indoor/low light\n",
    "        elif lighting == \"bright\":\n",
    "            return \"Blockbuster\"  # Vivid style for bright scenes\n",
    "\n",
    "        # Default fallback\n",
    "        return \"Auto-Enhance\"\n",
    "\n",
    "# Implementation of NLProcessor class\n",
    "class NLProcessor:\n",
    "    \"\"\"Processes natural language instructions for photo editing.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"Initialize the natural language processor.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration dictionary\n",
    "        \"\"\"\n",
    "        self.config = config or {}\n",
    "        self._validate_config()\n",
    "\n",
    "        # Function registry maps function names to actual functions\n",
    "        self.function_registry = {}\n",
    "\n",
    "    def _validate_config(self) -> None:\n",
    "        \"\"\"Validate and set default configuration parameters.\"\"\"\n",
    "        defaults = {\n",
    "            'api_key': os.environ.get('OPENAI_API_KEY', ''),\n",
    "            'model': 'gpt-4',\n",
    "            'max_tokens': 150,\n",
    "        }\n",
    "\n",
    "        for key, value in defaults.items():\n",
    "            if key not in self.config:\n",
    "                self.config[key] = value\n",
    "\n",
    "    def register_function(self, name: str, func: Callable, description: str, parameters: Dict[str, Any]):\n",
    "        \"\"\"Register a function that can be called from natural language.\n",
    "\n",
    "        Args:\n",
    "            name: Function name\n",
    "            func: Function to call\n",
    "            description: Description of what the function does\n",
    "            parameters: Parameter schema for the function\n",
    "        \"\"\"\n",
    "        self.function_registry[name] = {\n",
    "            'function': func,\n",
    "            'description': description,\n",
    "            'parameters': parameters\n",
    "        }\n",
    "\n",
    "    def process(self, image: np.ndarray, instruction: str) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "        \"\"\"Process a natural language instruction and apply it to an image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "            instruction: Natural language instruction\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (processed image, metadata)\n",
    "        \"\"\"\n",
    "        # Extract operations from instruction\n",
    "        operations = self._parse_instruction(instruction)\n",
    "\n",
    "        # Initialize metadata\n",
    "        metadata = {\n",
    "            'instruction': instruction,\n",
    "            'functions_called': [],\n",
    "            'errors': []\n",
    "        }\n",
    "\n",
    "        # Apply each operation in sequence\n",
    "        result = image.copy()\n",
    "        for op in operations:\n",
    "            try:\n",
    "                # Log the function call\n",
    "                metadata['functions_called'].append({\n",
    "                    'name': op['name'],\n",
    "                    'args': op['arguments']\n",
    "                })\n",
    "\n",
    "                # Call the function\n",
    "                if op['name'] in self.function_registry:\n",
    "                    func_info = self.function_registry[op['name']]\n",
    "                    func = func_info['function']\n",
    "                    result = func(result, **op['arguments'])\n",
    "                else:\n",
    "                    metadata['errors'].append(f\"Unknown function: {op['name']}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                metadata['errors'].append(f\"Error in {op['name']}: {str(e)}\")\n",
    "                logger.error(f\"Error applying operation {op['name']}: {e}\")\n",
    "\n",
    "        return result, metadata\n",
    "\n",
    "    def _parse_instruction(self, instruction: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Parse natural language instruction into specific operations.\n",
    "\n",
    "        Args:\n",
    "            instruction: Natural language instruction\n",
    "\n",
    "        Returns:\n",
    "            List of operations (function name and arguments)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # In a real implementation, this would call an LLM API with function calling\n",
    "            # For demo purposes, we'll simulate the function calling behavior\n",
    "\n",
    "            # Generate JSON schema for all registered functions\n",
    "            tools = []\n",
    "            for name, info in self.function_registry.items():\n",
    "                tools.append({\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": name,\n",
    "                        \"description\": info['description'],\n",
    "                        \"parameters\": info['parameters']\n",
    "                    }\n",
    "                })\n",
    "\n",
    "            # For demo purposes, let's simulate an LLM response based on the instruction\n",
    "            return self._simulate_function_calls(instruction)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing instruction: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _simulate_function_calls(self, instruction: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Simulate function calls based on the instruction (demo only).\n",
    "\n",
    "        Args:\n",
    "            instruction: Natural language instruction\n",
    "\n",
    "        Returns:\n",
    "            List of simulated function calls\n",
    "        \"\"\"\n",
    "        instruction_lower = instruction.lower()\n",
    "        operations = []\n",
    "\n",
    "        # Handle brightness/exposure adjustments\n",
    "        if any(term in instruction_lower for term in ['bright', 'exposure', 'darker', 'lighter']):\n",
    "            amount = 0.3 if 'bright' in instruction_lower or 'lighter' in instruction_lower else -0.3\n",
    "\n",
    "            # Adjust magnitude based on modifiers\n",
    "            if 'slightly' in instruction_lower or 'subtle' in instruction_lower:\n",
    "                amount *= 0.5\n",
    "            elif 'very' in instruction_lower or 'much' in instruction_lower:\n",
    "                amount *= 1.5\n",
    "\n",
    "            operations.append({\n",
    "                'name': 'adjust_exposure',\n",
    "                'arguments': {'amount': amount}\n",
    "            })\n",
    "\n",
    "        # Handle contrast adjustments\n",
    "        if 'contrast' in instruction_lower:\n",
    "            increase = 'increase' in instruction_lower or 'more' in instruction_lower\n",
    "            amount = 1.2 if increase else 0.8\n",
    "\n",
    "            # Adjust magnitude based on modifiers\n",
    "            if 'slightly' in instruction_lower or 'subtle' in instruction_lower:\n",
    "                amount = 1.1 if increase else 0.9\n",
    "            elif 'very' in instruction_lower or 'much' in instruction_lower or 'dramatic' in instruction_lower:\n",
    "                amount = 1.4 if increase else 0.7\n",
    "\n",
    "            operations.append({\n",
    "                'name': 'adjust_contrast',\n",
    "                'arguments': {'multiplier': amount}\n",
    "            })\n",
    "\n",
    "        # Handle saturation/vibrance adjustments\n",
    "        if any(term in instruction_lower for term in ['saturation', 'vibrance', 'vibrant', 'colorful']):\n",
    "            increase = not ('reduce' in instruction_lower or 'less' in instruction_lower)\n",
    "            amount = 0.2 if increase else -0.2\n",
    "\n",
    "            # Adjust magnitude based on modifiers\n",
    "            if 'slightly' in instruction_lower or 'subtle' in instruction_lower:\n",
    "                amount *= 0.5\n",
    "            elif 'very' in instruction_lower or 'much' in instruction_lower:\n",
    "                amount *= 1.5\n",
    "\n",
    "            operations.append({\n",
    "                'name': 'adjust_saturation',\n",
    "                'arguments': {'adjustment': amount}\n",
    "            })\n",
    "\n",
    "        # Handle temperature/warmth adjustments\n",
    "        if any(term in instruction_lower for term in ['warm', 'temperature', 'cool', 'cold']):\n",
    "            warm = 'warm' in instruction_lower\n",
    "            amount = 0.15 if warm else -0.15\n",
    "\n",
    "            # Adjust magnitude based on modifiers\n",
    "            if 'slightly' in instruction_lower or 'subtle' in instruction_lower:\n",
    "                amount *= 0.5\n",
    "            elif 'very' in instruction_lower or 'much' in instruction_lower:\n",
    "                amount *= 1.5\n",
    "\n",
    "            operations.append({\n",
    "                'name': 'adjust_temperature',\n",
    "                'arguments': {'adjustment': amount}\n",
    "            })\n",
    "\n",
    "        # Handle sharpness adjustments\n",
    "        if any(term in instruction_lower for term in ['sharp', 'clarity', 'detail']):\n",
    "            increase = not ('reduce' in instruction_lower or 'less' in instruction_lower)\n",
    "            amount = 0.3 if increase else -0.1\n",
    "\n",
    "            # Adjust magnitude based on modifiers\n",
    "            if 'slightly' in instruction_lower or 'subtle' in instruction_lower:\n",
    "                amount *= 0.7\n",
    "            elif 'very' in instruction_lower or 'much' in instruction_lower:\n",
    "                amount *= 1.5\n",
    "\n",
    "            operations.append({\n",
    "                'name': 'adjust_sharpness',\n",
    "                'arguments': {'strength': max(0, amount)}\n",
    "            })\n",
    "\n",
    "        # Handle noise reduction\n",
    "        if 'noise' in instruction_lower or 'grain' in instruction_lower:\n",
    "            reduce = 'reduce' in instruction_lower or 'less' in instruction_lower or 'remove' in instruction_lower\n",
    "            amount = 0.4 if reduce else 0.1\n",
    "\n",
    "            # Adjust magnitude based on modifiers\n",
    "            if 'slightly' in instruction_lower or 'subtle' in instruction_lower:\n",
    "                amount *= 0.7\n",
    "            elif 'very' in instruction_lower or 'much' in instruction_lower:\n",
    "                amount *= 1.5\n",
    "\n",
    "            operations.append({\n",
    "                'name': 'reduce_noise',\n",
    "                'arguments': {'strength': amount}\n",
    "            })\n",
    "\n",
    "        # Handle style-based instructions\n",
    "        if 'cinematic' in instruction_lower:\n",
    "            if 'dramatic' in instruction_lower or 'dark' in instruction_lower:\n",
    "                operations.append({\n",
    "                    'name': 'apply_style',\n",
    "                    'arguments': {'style_name': 'Film Noir'}\n",
    "                })\n",
    "            elif 'anamorphic' in instruction_lower or 'widescreen' in instruction_lower:\n",
    "                operations.append({\n",
    "                    'name': 'apply_style',\n",
    "                    'arguments': {'style_name': 'Anamorphic'}\n",
    "                })\n",
    "            else:\n",
    "                operations.append({\n",
    "                    'name': 'apply_style',\n",
    "                    'arguments': {'style_name': 'Cinematic Teal & Orange'}\n",
    "                })\n",
    "        elif 'vintage' in instruction_lower or 'retro' in instruction_lower:\n",
    "            operations.append({\n",
    "                'name': 'apply_style',\n",
    "                'arguments': {'style_name': 'Vintage'}\n",
    "            })\n",
    "        elif 'portrait' in instruction_lower:\n",
    "            operations.append({\n",
    "                'name': 'apply_style',\n",
    "                'arguments': {'style_name': 'Portrait'}\n",
    "            })\n",
    "        elif 'dreamy' in instruction_lower or 'soft' in instruction_lower:\n",
    "            operations.append({\n",
    "                'name': 'apply_style',\n",
    "                'arguments': {'style_name': 'Dreamy'}\n",
    "            })\n",
    "        elif 'dramatic' in instruction_lower or 'action' in instruction_lower:\n",
    "            operations.append({\n",
    "                'name': 'apply_style',\n",
    "                'arguments': {'style_name': 'Blockbuster'}\n",
    "            })\n",
    "\n",
    "        # If no specific operations were identified, apply auto-enhance\n",
    "        if not operations:\n",
    "            operations.append({\n",
    "                'name': 'apply_style',\n",
    "                'arguments': {'style_name': 'Auto-Enhance'}\n",
    "            })\n",
    "\n",
    "        return operations\n",
    "\n",
    "# Implementation of RAGStyleEngine class\n",
    "class RAGStyleEngine:\n",
    "    \"\"\"Recommends and applies styles using RAG techniques.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"Initialize the RAG style engine.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration dictionary\n",
    "        \"\"\"\n",
    "        self.config = config or {}\n",
    "        self._validate_config()\n",
    "\n",
    "        # Initialize image analyzer\n",
    "        self.image_analyzer = AIImageAnalyzer()\n",
    "\n",
    "        # Initialize knowledge base\n",
    "        self.knowledge_base = self._init_knowledge_base()\n",
    "\n",
    "        # Initialize embedding database\n",
    "        self.embedding_db = None\n",
    "        self._init_embedding_database()\n",
    "\n",
    "    def _validate_config(self) -> None:\n",
    "        \"\"\"Validate and set default configuration parameters.\"\"\"\n",
    "        defaults = {\n",
    "            'knowledge_base_path': os.path.join(os.path.dirname(__file__), 'data', 'style_knowledge.json'),\n",
    "            'embedding_db_path': os.path.join(os.path.dirname(__file__), 'data', 'style_embeddings.npz'),\n",
    "            'custom_styles_path': os.path.join(os.path.dirname(__file__), 'data', 'custom_styles'),\n",
    "        }\n",
    "\n",
    "        for key, value in defaults.items():\n",
    "            if key not in self.config:\n",
    "                self.config[key] = value\n",
    "\n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(os.path.dirname(self.config['knowledge_base_path']), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(self.config['embedding_db_path']), exist_ok=True)\n",
    "        os.makedirs(self.config['custom_styles_path'], exist_ok=True)\n",
    "\n",
    "    def _init_knowledge_base(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Initialize the style knowledge base.\n",
    "\n",
    "        Returns:\n",
    "            List of style knowledge entries\n",
    "        \"\"\"\n",
    "        # Default knowledge base with cinematography styles and techniques\n",
    "        default_knowledge = [\n",
    "            {\n",
    "                \"style_name\": \"Cinematic Teal & Orange\",\n",
    "                \"description\": \"Classic Hollywood color grading with teal shadows and orange highlights\",\n",
    "                \"keywords\": [\"blockbuster\", \"cinematic\", \"movie\", \"film\", \"hollywood\", \"complementary\"],\n",
    "                \"examples\": [\"Transformers\", \"Marvel movies\", \"Michael Bay\", \"action films\"],\n",
    "                \"techniques\": [\"Shadow/highlight color split\", \"Blue shadows, orange highlights\", \"High contrast\"]\n",
    "            },\n",
    "            {\n",
    "                \"style_name\": \"Film Noir\",\n",
    "                \"description\": \"High contrast black and white with dramatic shadows and film grain\",\n",
    "                \"keywords\": [\"noir\", \"detective\", \"dark\", \"mysterious\", \"contrast\", \"dramatic\", \"moody\"],\n",
    "                \"examples\": [\"The Maltese Falcon\", \"Citizen Kane\", \"Double Indemnity\"],\n",
    "                \"techniques\": [\"Hard shadows\", \"Low-key lighting\", \"Strong contrast\", \"Moody atmosphere\"]\n",
    "            },\n",
    "            {\n",
    "                \"style_name\": \"Anamorphic\",\n",
    "                \"description\": \"Widescreen cinematic look with lens flares and letterboxing\",\n",
    "                \"keywords\": [\"widescreen\", \"anamorphic\", \"cinematic\", \"lens flare\", \"letterbox\"],\n",
    "                \"examples\": [\"JJ Abrams films\", \"Star Trek\", \"Star Wars\", \"Sci-fi films\"],\n",
    "                \"techniques\": [\"Horizontal lens flares\", \"Wide aspect ratio\", \"Anamorphic lens distortion\"]\n",
    "            },\n",
    "            {\n",
    "                \"style_name\": \"Blockbuster\",\n",
    "                \"description\": \"Vibrant colors and high contrast for modern action films\",\n",
    "                \"keywords\": [\"action\", \"vibrant\", \"punchy\", \"bright\", \"dramatic\", \"dynamic\"],\n",
    "                \"examples\": [\"Fast & Furious\", \"Mission Impossible\", \"Modern action films\"],\n",
    "                \"techniques\": [\"Increased saturation\", \"High contrast\", \"Sharper details\", \"Vibrant colors\"]\n",
    "            },\n",
    "            {\n",
    "                \"style_name\": \"Dreamy\",\n",
    "                \"description\": \"Soft, ethereal look with warm tones and gentle glow\",\n",
    "                \"keywords\": [\"soft\", \"ethereal\", \"dreamy\", \"romantic\", \"fantasy\", \"peaceful\"],\n",
    "                \"examples\": [\"Romance films\", \"Fantasy sequences\", \"Music videos\"],\n",
    "                \"techniques\": [\"Soft focus\", \"Glow effect\", \"Reduced contrast\", \"Warm tones\"]\n",
    "            },\n",
    "            {\n",
    "                \"style_name\": \"Vintage\",\n",
    "                \"description\": \"Classic film-inspired look with faded colors and subtle vignette\",\n",
    "                \"keywords\": [\"retro\", \"vintage\", \"old\", \"classic\", \"film\", \"nostalgic\"],\n",
    "                \"examples\": [\"Old photographs\", \"Analog film\", \"Instagram filters\"],\n",
    "                \"techniques\": [\"Sepia tones\", \"Reduced contrast\", \"Faded blacks\", \"Vignette\"]\n",
    "            },\n",
    "            {\n",
    "                \"style_name\": \"Portrait\",\n",
    "                \"description\": \"Optimized for portrait photography with skin tone enhancement\",\n",
    "                \"keywords\": [\"portrait\", \"person\", \"face\", \"skin\", \"beauty\", \"professional\"],\n",
    "                \"examples\": [\"Professional portraits\", \"Headshots\", \"Fashion photography\"],\n",
    "                \"techniques\": [\"Skin smoothing\", \"Warm tones\", \"Subtle contrast\", \"Detail preservation\"]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Try to load existing knowledge base\n",
    "        try:\n",
    "            if os.path.exists(self.config['knowledge_base_path']):\n",
    "                with open(self.config['knowledge_base_path'], 'r') as f:\n",
    "                    knowledge = json.load(f)\n",
    "                    logger.info(f\"Loaded {len(knowledge)} style knowledge entries\")\n",
    "                    return knowledge\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading knowledge base: {e}\")\n",
    "\n",
    "        # If loading fails, use default and save it\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(self.config['knowledge_base_path']), exist_ok=True)\n",
    "            with open(self.config['knowledge_base_path'], 'w') as f:\n",
    "                json.dump(default_knowledge, f, indent=2)\n",
    "                logger.info(f\"Created default knowledge base with {len(default_knowledge)} entries\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving knowledge base: {e}\")\n",
    "\n",
    "        return default_knowledge\n",
    "\n",
    "    def _init_embedding_database(self) -> None:\n",
    "        \"\"\"Initialize the embedding database.\n",
    "\n",
    "        In a real implementation, this would create embeddings for all style knowledge.\n",
    "        For the MVP, we'll simulate embeddings.\n",
    "        \"\"\"\n",
    "        # Attempt to load existing embeddings\n",
    "        try:\n",
    "            if os.path.exists(self.config['embedding_db_path']):\n",
    "                # In a real implementation, load saved embeddings\n",
    "                logger.info(\"Loaded style embeddings\")\n",
    "                self.embedding_db = True  # Placeholder, simulating successful loading\n",
    "                return\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading embeddings: {e}\")\n",
    "\n",
    "        # If loading fails, generate simulated embeddings\n",
    "        try:\n",
    "            # In a real implementation, generate embeddings for all entries\n",
    "            logger.info(\"Generating simulated style embeddings\")\n",
    "            self.embedding_db = True  # Placeholder, simulating successful generation\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating embeddings: {e}\")\n",
    "\n",
    "    def recommend_style(self, image: np.ndarray, description: Optional[str] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Recommend styles based on image content and optional description.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "            description: Optional user description of desired style\n",
    "\n",
    "        Returns:\n",
    "            List of recommended styles with reasoning\n",
    "        \"\"\"\n",
    "        # Analyze image content\n",
    "        _, analysis = self.image_analyzer.analyze(image)\n",
    "\n",
    "        # Extract key features from analysis\n",
    "        scene_type = analysis.get('scene_type', 'unknown')\n",
    "        has_faces = analysis.get('has_faces', False)\n",
    "        lighting = analysis.get('lighting_condition', 'normal')\n",
    "\n",
    "        # Match against style knowledge\n",
    "        if description:\n",
    "            # For a real implementation, we would use embeddings to find similar styles\n",
    "            # For now, we'll use keyword matching\n",
    "            return self._match_by_description(description, scene_type, has_faces, lighting)\n",
    "        else:\n",
    "            # Content-based recommendation\n",
    "            return self._match_by_content(scene_type, has_faces, lighting)\n",
    "\n",
    "    def _match_by_description(self, description: str, scene_type: str, has_faces: bool, lighting: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Match styles based on user description and image content.\n",
    "\n",
    "        Args:\n",
    "            description: User description of desired style\n",
    "            scene_type: Detected scene type\n",
    "            has_faces: Whether faces were detected\n",
    "            lighting: Detected lighting condition\n",
    "\n",
    "        Returns:\n",
    "            List of recommended styles with reasoning\n",
    "        \"\"\"\n",
    "        description_lower = description.lower()\n",
    "        matches = []\n",
    "\n",
    "        # Score each style entry\n",
    "        for entry in self.knowledge_base:\n",
    "            score = 0\n",
    "            reasoning = []\n",
    "\n",
    "            # Match based on style name\n",
    "            if entry['style_name'].lower() in description_lower:\n",
    "                score += 10\n",
    "                reasoning.append(f\"Explicitly mentioned {entry['style_name']}\")\n",
    "\n",
    "            # Match based on keywords\n",
    "            for keyword in entry.get('keywords', []):\n",
    "                if keyword.lower() in description_lower:\n",
    "                    score += 3\n",
    "                    reasoning.append(f\"Mentioned '{keyword}'\")\n",
    "\n",
    "            # Match based on examples\n",
    "            for example in entry.get('examples', []):\n",
    "                if example.lower() in description_lower:\n",
    "                    score += 5\n",
    "                    reasoning.append(f\"Referenced '{example}'\")\n",
    "\n",
    "            # Match based on techniques\n",
    "            for technique in entry.get('techniques', []):\n",
    "                tech_words = set(technique.lower().split())\n",
    "                if any(word in description_lower for word in tech_words):\n",
    "                    score += 2\n",
    "                    reasoning.append(f\"Described technique similar to '{technique}'\")\n",
    "\n",
    "            # Content-based boosting\n",
    "            if scene_type == \"landscape\" and entry['style_name'] in [\"Cinematic Teal & Orange\", \"Anamorphic\"]:\n",
    "                score += 2\n",
    "                reasoning.append(\"Good match for landscape content\")\n",
    "\n",
    "            if has_faces and entry['style_name'] in [\"Portrait\", \"Film Noir\"]:\n",
    "                score += 2\n",
    "                reasoning.append(\"Good match for portrait content\")\n",
    "\n",
    "            if lighting == \"low_light\" and entry['style_name'] in [\"Film Noir\", \"Anamorphic\"]:\n",
    "                score += 2\n",
    "                reasoning.append(\"Good match for low-light conditions\")\n",
    "\n",
    "            # Add to matches if above threshold\n",
    "            if score > 0:\n",
    "                matches.append({\n",
    "                    'style': entry['style_name'],\n",
    "                    'description': entry['description'],\n",
    "                    'score': score,\n",
    "                    'reasoning': reasoning\n",
    "                })\n",
    "\n",
    "        # Sort by score and return top matches\n",
    "        matches.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return matches[:3]  # Return top 3 matches\n",
    "\n",
    "    def _match_by_content(self, scene_type: str, has_faces: bool, lighting: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Match styles based on image content.\n",
    "\n",
    "        Args:\n",
    "            scene_type: Detected scene type\n",
    "            has_faces: Whether faces were detected\n",
    "            lighting: Detected lighting condition\n",
    "\n",
    "        Returns:\n",
    "            List of recommended styles with reasoning\n",
    "        \"\"\"\n",
    "        matches = []\n",
    "\n",
    "        # Content-specific recommendations\n",
    "        if has_faces:\n",
    "            if lighting == \"low_light\":\n",
    "                # Portrait in low light\n",
    "                matches.append({\n",
    "                    'style': \"Film Noir\",\n",
    "                    'description': \"High contrast black and white with dramatic shadows\",\n",
    "                    'score': 9,\n",
    "                    'reasoning': [\"Detected faces in low light\", \"Dramatic lighting suits noir style\"]\n",
    "                })\n",
    "            else:\n",
    "                # Standard portrait\n",
    "                matches.append({\n",
    "                    'style': \"Portrait\",\n",
    "                    'description': \"Optimized for portrait photography with skin tone enhancement\",\n",
    "                    'score': 9,\n",
    "                    'reasoning': [\"Detected faces\", \"Standard portrait enhancement\"]\n",
    "                })\n",
    "\n",
    "        if scene_type in [\"landscape\", \"nature\"]:\n",
    "            # Landscape scene\n",
    "            matches.append({\n",
    "                'style': \"Cinematic Teal & Orange\",\n",
    "                'description': \"Hollywood-style color grading with teal shadows and orange highlights\",\n",
    "                'score': 8,\n",
    "                'reasoning': [\"Detected landscape/nature scene\", \"Popular cinematic look for landscapes\"]\n",
    "            })\n",
    "\n",
    "            matches.append({\n",
    "                'style': \"Anamorphic\",\n",
    "                'description': \"Widescreen cinematic look with enhanced contrast and blue lens flares\",\n",
    "                'score': 7,\n",
    "                'reasoning': [\"Detected landscape/nature scene\", \"Wide cinematic style suits landscape views\"]\n",
    "            })\n",
    "\n",
    "        if lighting == \"low_light\":\n",
    "            # Low light scene\n",
    "            if not has_faces and scene_type not in [\"landscape\", \"nature\"]:\n",
    "                matches.append({\n",
    "                    'style': \"Anamorphic\",\n",
    "                    'description': \"Widescreen cinematic look with enhanced contrast and blue lens flares\",\n",
    "                    'score': 6,\n",
    "                    'reasoning': [\"Low light scene detected\", \"Cinematic look enhances mood\"]\n",
    "                })\n",
    "\n",
    "        if scene_type == \"indoor\":\n",
    "            # Indoor scene\n",
    "            matches.append({\n",
    "                'style': \"Vintage\",\n",
    "                'description': \"Classic film-inspired look with faded colors\",\n",
    "                'score': 5,\n",
    "                'reasoning': [\"Detected indoor scene\", \"Vintage style works well with indoor settings\"]\n",
    "            })\n",
    "\n",
    "        # Add auto-enhance as a fallback\n",
    "        if not matches:\n",
    "            matches.append({\n",
    "                'style': \"Auto-Enhance\",\n",
    "                'description': \"Balanced automatic enhancement for most photos\",\n",
    "                'score': 5,\n",
    "                'reasoning': [\"General purpose enhancement\"]\n",
    "            })\n",
    "\n",
    "        # Sort by score\n",
    "        matches.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return matches[:3]  # Return top 3 matches\n",
    "\n",
    "    def apply_style(self, image: np.ndarray, style_name: str) -> np.ndarray:\n",
    "        \"\"\"Apply the specified style to an image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image\n",
    "            style_name: Name of the style to apply\n",
    "\n",
    "        Returns:\n",
    "            Processed image\n",
    "        \"\"\"\n",
    "        # Import here to avoid circular imports\n",
    "        executor = ImageExecutor()\n",
    "\n",
    "        # Apply the style using the executor\n",
    "        processed = executor.apply(image, [], style_name)\n",
    "\n",
    "        return processed\n"
   ],
   "id": "dd661636c3acc526"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadae7799c7ebe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image, title=None):\n",
    "    \"\"\"Display an image with an optional title.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if isinstance(image, str):\n",
    "        # If image is a file path, load it\n",
    "        image = load_image(image)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "def display_before_after(before, after, titles=None):\n",
    "    \"\"\"Display before and after images side by side.\"\"\"\n",
    "    if titles is None:\n",
    "        titles = ['Before', 'After']\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if isinstance(before, str):\n",
    "        before = load_image(before)\n",
    "    plt.imshow(before)\n",
    "    plt.axis('off')\n",
    "    plt.title(titles[0], fontsize=14)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if isinstance(after, str):\n",
    "        after = load_image(after)\n",
    "    plt.imshow(after)\n",
    "    plt.axis('off')\n",
    "    plt.title(titles[1], fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_multiple(images, titles=None, cols=3):\n",
    "    \"\"\"Display multiple images in a grid.\"\"\"\n",
    "    n = len(images)\n",
    "    rows = (n + cols - 1) // cols\n",
    "\n",
    "    plt.figure(figsize=(5*cols, 5*rows))\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        if isinstance(image, str):\n",
    "            image = load_image(image)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        if titles and i < len(titles):\n",
    "            plt.title(titles[i], fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7867bc1684f40",
   "metadata": {},
   "source": [
    "## Load Test Image\n",
    "\n",
    "Let's load a test image that we'll use throughout this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4089e519306e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a test image from the web\n",
    "test_image_url = 'https://raw.githubusercontent.com/opencv/opencv/master/samples/data/lena.jpg'\n",
    "image = load_image(test_image_url)\n",
    "# Note: Using a sample image from OpenCV's GitHub repository (Lena image)\n",
    "\n",
    "# Display the image\n",
    "display_image(image, \"Test Image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54861e9f633a15",
   "metadata": {},
   "source": [
    "# Part 1: AI-Powered Image Analysis\n",
    "\n",
    "One of the key innovations in our photo editor is the ability to analyze image content using AI. This allows the application to understand what's in the photo and make intelligent recommendations based on the content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d83cc65a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AI image analyzer\n",
    "ai_analyzer = AIImageAnalyzer()\n",
    "\n",
    "# Analyze the image\n",
    "adjustments, analysis = ai_analyzer.analyze(image)\n",
    "\n",
    "# Display the analysis results\n",
    "print(\"AI Image Analysis Results:\")\n",
    "print(f\"Scene Type: {analysis['scene_type']}\")\n",
    "print(f\"Lighting Condition: {analysis['lighting_condition']}\")\n",
    "print(f\"Faces Detected: {analysis['face_count']}\")\n",
    "print(\"\\nDetected Objects:\")\n",
    "for obj in analysis['objects']:\n",
    "    print(f\"- {obj['class']} (confidence: {obj['confidence']:.2f})\")\n",
    "\n",
    "print(\"\\nDominant Colors:\")\n",
    "for i, color in enumerate(analysis['color_palette'][:3]):\n",
    "    print(f\"- Color {i+1}: RGB{tuple(color)}\")\n",
    "\n",
    "print(\"\\nRecommended Adjustments:\")\n",
    "for adj in adjustments:\n",
    "    print(f\"- {adj.parameter}: {adj.suggested} {adj.unit} - {adj.description}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450b2cc5409a99a",
   "metadata": {},
   "source": [
    "### Visualizing the AI Analysis\n",
    "\n",
    "Let's visualize some of the analysis results to better understand what the AI is seeing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c825f5570ce059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the color palette\n",
    "def display_color_palette(colors):\n",
    "    \"\"\"Display the color palette as color swatches.\"\"\"\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i, color in enumerate(colors):\n",
    "        plt.subplot(1, len(colors), i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow([[color]])\n",
    "        plt.title(f\"RGB{tuple(color)}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Dominant Color Palette:\")\n",
    "display_color_palette(analysis['color_palette'][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eeb8e5f12cc8f9",
   "metadata": {},
   "source": [
    "### Applying AI-Recommended Adjustments\n",
    "\n",
    "Now that we have AI-recommended adjustments, let's apply them to the image and see the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ed99458e9b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the recommended adjustments\n",
    "adjusted_image = apply_adjustments(image, adjustments)\n",
    "\n",
    "# Display before and after\n",
    "display_before_after(image, adjusted_image, [\"Original Image\", \"AI-Enhanced Image\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15893e292fc2030d",
   "metadata": {},
   "source": [
    "# Part 2: Natural Language Photo Editing\n",
    "\n",
    "Another innovative feature of our photo editor is the ability to edit photos using natural language instructions. This allows users to describe what they want in plain English, without needing to understand technical terms or complex editing tools.\n",
    "\n",
    "## Image Processing Functions\n",
    "\n",
    "Let's implement the image processing functions that our natural language processor will use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7140e66022bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_exposure(image, amount):\n",
    "    \"\"\"Adjust image exposure/brightness.\n",
    "\n",
    "    Args:\n",
    "        image: Input image\n",
    "        amount: Adjustment amount (-1.0 to 1.0)\n",
    "\n",
    "    Returns:\n",
    "        Adjusted image\n",
    "    \"\"\"\n",
    "    # Simple implementation for demonstration\n",
    "    result = image.copy().astype(float)\n",
    "    result = result * (1 + amount)\n",
    "    return np.clip(result, 0, 255).astype(np.uint8)\n",
    "\n",
    "def adjust_contrast(image, multiplier):\n",
    "    \"\"\"Adjust image contrast.\n",
    "\n",
    "    Args:\n",
    "        image: Input image\n",
    "        multiplier: Contrast multiplier (0.5 to 2.0)\n",
    "\n",
    "    Returns:\n",
    "        Adjusted image\n",
    "    \"\"\"\n",
    "    # Simple implementation for demonstration\n",
    "    mean = np.mean(image, axis=(0, 1))\n",
    "    result = image.copy().astype(float)\n",
    "    for i in range(3):\n",
    "        result[:,:,i] = (result[:,:,i] - mean[i]) * multiplier + mean[i]\n",
    "    return np.clip(result, 0, 255).astype(np.uint8)\n",
    "\n",
    "def adjust_saturation(image, adjustment):\n",
    "    \"\"\"Adjust image saturation/vibrance.\n",
    "\n",
    "    Args:\n",
    "        image: Input image\n",
    "        adjustment: Saturation adjustment (-1.0 to 1.0)\n",
    "\n",
    "    Returns:\n",
    "        Adjusted image\n",
    "    \"\"\"\n",
    "    # Convert to HSV, adjust S channel, convert back to RGB\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype(float)\n",
    "    hsv[:,:,1] = hsv[:,:,1] * (1 + adjustment)\n",
    "    hsv[:,:,1] = np.clip(hsv[:,:,1], 0, 255)\n",
    "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "\n",
    "def adjust_temperature(image, adjustment):\n",
    "    \"\"\"Adjust image color temperature (warmth/coolness).\n",
    "\n",
    "    Args:\n",
    "        image: Input image\n",
    "        adjustment: Temperature adjustment (-0.5 to 0.5)\n",
    "\n",
    "    Returns:\n",
    "        Adjusted image\n",
    "    \"\"\"\n",
    "    # Simple implementation - increase red for warmth, blue for coolness\n",
    "    result = image.copy().astype(float)\n",
    "    if adjustment > 0:  # Warm\n",
    "        result[:,:,0] = np.clip(result[:,:,0] * (1 + adjustment), 0, 255)  # Red\n",
    "        result[:,:,2] = np.clip(result[:,:,2] * (1 - adjustment/2), 0, 255)  # Blue\n",
    "    else:  # Cool\n",
    "        result[:,:,2] = np.clip(result[:,:,2] * (1 - adjustment), 0, 255)  # Blue\n",
    "        result[:,:,0] = np.clip(result[:,:,0] * (1 + adjustment/2), 0, 255)  # Red\n",
    "    return result.astype(np.uint8)\n",
    "\n",
    "def adjust_sharpness(image, strength):\n",
    "    \"\"\"Adjust image sharpness.\n",
    "\n",
    "    Args:\n",
    "        image: Input image\n",
    "        strength: Sharpness strength (0.0 to 1.0)\n",
    "\n",
    "    Returns:\n",
    "        Adjusted image\n",
    "    \"\"\"\n",
    "    # Simple implementation using unsharp masking\n",
    "    blur = cv2.GaussianBlur(image, (0, 0), 3)\n",
    "    result = image.copy().astype(float)\n",
    "    result = result + strength * (image.astype(float) - blur)\n",
    "    return np.clip(result, 0, 255).astype(np.uint8)\n",
    "\n",
    "def reduce_noise(image, strength):\n",
    "    \"\"\"Reduce noise in the image.\n",
    "\n",
    "    Args:\n",
    "        image: Input image\n",
    "        strength: Noise reduction strength (0.0 to 1.0)\n",
    "\n",
    "    Returns:\n",
    "        Adjusted image\n",
    "    \"\"\"\n",
    "    # Simple implementation using bilateral filter\n",
    "    # Adjust parameters based on strength\n",
    "    d = int(5 + strength * 10)  # Diameter of each pixel neighborhood\n",
    "    sigma_color = 50 + strength * 100  # Filter sigma in the color space\n",
    "    sigma_space = 50 + strength * 100  # Filter sigma in the coordinate space\n",
    "\n",
    "    return cv2.bilateralFilter(image, d, sigma_color, sigma_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a756c178c9a6393",
   "metadata": {},
   "source": [
    "## Setting Up the Natural Language Processor\n",
    "\n",
    "Now let's set up our natural language processor and register the image processing functions we defined above. This will allow the processor to map natural language instructions to specific operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d7e642c448c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the natural language processor\n",
    "nl_processor = NLProcessor()\n",
    "\n",
    "# Register our image processing functions\n",
    "nl_processor.register_function(\n",
    "    \"adjust_exposure\",\n",
    "    adjust_exposure,\n",
    "    \"Adjust the brightness/exposure of the image\",\n",
    "    {\"amount\": {\"type\": \"number\", \"description\": \"Amount to adjust exposure (-1.0 to 1.0)\"}}\n",
    ")\n",
    "\n",
    "nl_processor.register_function(\n",
    "    \"adjust_contrast\",\n",
    "    adjust_contrast,\n",
    "    \"Adjust the contrast of the image\",\n",
    "    {\"multiplier\": {\"type\": \"number\", \"description\": \"Contrast multiplier (0.5 to 2.0)\"}}\n",
    ")\n",
    "\n",
    "nl_processor.register_function(\n",
    "    \"adjust_saturation\",\n",
    "    adjust_saturation,\n",
    "    \"Adjust the color saturation/vibrance of the image\",\n",
    "    {\"adjustment\": {\"type\": \"number\", \"description\": \"Saturation adjustment (-1.0 to 1.0)\"}}\n",
    ")\n",
    "\n",
    "nl_processor.register_function(\n",
    "    \"adjust_temperature\",\n",
    "    adjust_temperature,\n",
    "    \"Adjust the color temperature (warmth/coolness) of the image\",\n",
    "    {\"adjustment\": {\"type\": \"number\", \"description\": \"Temperature adjustment (-0.5 to 0.5)\"}}\n",
    ")\n",
    "\n",
    "nl_processor.register_function(\n",
    "    \"adjust_sharpness\",\n",
    "    adjust_sharpness,\n",
    "    \"Adjust the sharpness/clarity of the image\",\n",
    "    {\"strength\": {\"type\": \"number\", \"description\": \"Sharpness strength (0.0 to 1.0)\"}}\n",
    ")\n",
    "\n",
    "nl_processor.register_function(\n",
    "    \"reduce_noise\",\n",
    "    reduce_noise,\n",
    "    \"Reduce noise/grain in the image\",\n",
    "    {\"strength\": {\"type\": \"number\", \"description\": \"Noise reduction strength (0.0 to 1.0)\"}}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72077004c3eebee",
   "metadata": {},
   "source": [
    "## Processing Natural Language Instructions\n",
    "\n",
    "Let's try some natural language instructions and see how the system interprets and applies them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a22141fd5723a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process and display results\n",
    "def process_instruction(image, instruction):\n",
    "    \"\"\"Process a natural language instruction and display results.\"\"\"\n",
    "    print(f\"Instruction: '{instruction}'\")\n",
    "\n",
    "    # Process the instruction\n",
    "    processed_image, metadata = nl_processor.process(image, instruction)\n",
    "\n",
    "    # Display the functions that were called\n",
    "    print(\"\\nFunctions called:\")\n",
    "    for func_call in metadata['functions_called']:\n",
    "        print(f\"- {func_call['name']}({', '.join([f'{k}={v}' for k, v in func_call['args'].items()])})\")\n",
    "\n",
    "    # Display before and after\n",
    "    display_before_after(image, processed_image, [\"Original Image\", f\"After: '{instruction}'\"])\n",
    "\n",
    "    return processed_image\n",
    "\n",
    "# Try a simple instruction\n",
    "result1 = process_instruction(image, \"Make the image warmer and increase the contrast slightly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2b2b857ba9b4a3",
   "metadata": {},
   "source": [
    "Let's try some more complex instructions to see how the system handles them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086fd6dec2ff158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try more complex instructions\n",
    "instructions = [\n",
    "    \"Make the colors more vibrant and add some warmth\",\n",
    "    \"Increase contrast dramatically and make it cooler\",\n",
    "    \"Brighten the dark areas and add clarity\",\n",
    "    \"Give it a soft, dreamy look with reduced contrast\",\n",
    "    \"Sharpen the details and make colors pop\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for instruction in instructions:\n",
    "    print(f\"\\n{'='*50}\\n\")\n",
    "    result = process_instruction(image, instruction)\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f624ef4ab932aecb",
   "metadata": {},
   "source": [
    "## Conversational Editing Workflow\n",
    "\n",
    "One of the most powerful applications of natural language photo editing is the ability to guide users through an iterative editing process, similar to working with a professional photo editor. Let's demonstrate this conversational editing workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb15dcf9e3df1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversational_editing_workflow(image):\n",
    "    \"\"\"Demonstrate a conversational editing workflow.\"\"\"\n",
    "    print(\"=== Conversational Photo Editing Workflow ===\\n\")\n",
    "    print(\"Starting with the original image:\")\n",
    "    display_image(image, \"Original Image\")\n",
    "\n",
    "    # Step 1: Initial assessment and basic enhancement\n",
    "    print(\"\\nStep 1: Initial assessment and basic enhancement\")\n",
    "    print(\"User: \\\"Enhance this photo to make it look better overall\\\"\")\n",
    "\n",
    "    current_image, metadata = nl_processor.process(image, \"Enhance this photo to make it look better overall\")\n",
    "\n",
    "    print(\"\\nAI: \\\"I've made some basic enhancements. I've slightly increased the exposure, added a bit of contrast, and made the colors more vibrant. Here's the result:\\\"\")\n",
    "    print(\"\\nOperations performed:\")\n",
    "    for func_call in metadata['functions_called']:\n",
    "        print(f\"- {func_call['name']}({', '.join([f'{k}={v}' for k, v in func_call['args'].items()])})\")\n",
    "\n",
    "    display_image(current_image, \"After Basic Enhancement\")\n",
    "\n",
    "    # Step 2: Specific adjustment based on user feedback\n",
    "    print(\"\\nStep 2: Specific adjustment based on user feedback\")\n",
    "    print(\"User: \\\"It looks better, but I'd like it to be a bit warmer and more dramatic\\\"\")\n",
    "\n",
    "    previous_image = current_image.copy()\n",
    "    current_image, metadata = nl_processor.process(current_image, \"Make it warmer and more dramatic\")\n",
    "\n",
    "    print(\"\\nAI: \\\"I've added warmth by adjusting the color temperature and increased the contrast for a more dramatic look. Here's the updated image:\\\"\")\n",
    "    print(\"\\nOperations performed:\")\n",
    "    for func_call in metadata['functions_called']:\n",
    "        print(f\"- {func_call['name']}({', '.join([f'{k}={v}' for k, v in func_call['args'].items()])})\")\n",
    "\n",
    "    display_before_after(previous_image, current_image, [\"After Basic Enhancement\", \"Warmer and More Dramatic\"])\n",
    "\n",
    "    # Step 3: Fine-tuning\n",
    "    print(\"\\nStep 3: Fine-tuning\")\n",
    "    print(\"User: \\\"That's closer to what I want, but now the colors are a bit too intense. Can you tone down the saturation slightly but keep the contrast?\\\"\")\n",
    "\n",
    "    previous_image = current_image.copy()\n",
    "    current_image, metadata = nl_processor.process(current_image, \"Reduce saturation slightly but maintain contrast\")\n",
    "\n",
    "    print(\"\\nAI: \\\"I've reduced the color saturation while maintaining the contrast levels. Here's the result:\\\"\")\n",
    "    print(\"\\nOperations performed:\")\n",
    "    for func_call in metadata['functions_called']:\n",
    "        print(f\"- {func_call['name']}({', '.join([f'{k}={v}' for k, v in func_call['args'].items()])})\")\n",
    "\n",
    "    display_before_after(previous_image, current_image, [\"Warmer and More Dramatic\", \"Fine-tuned\"])\n",
    "\n",
    "    # Step 4: Final touches\n",
    "    print(\"\\nStep 4: Final touches\")\n",
    "    print(\"User: \\\"That's looking good! As a final touch, can you sharpen it a bit to bring out the details?\\\"\")\n",
    "\n",
    "    previous_image = current_image.copy()\n",
    "    current_image, metadata = nl_processor.process(current_image, \"Sharpen to bring out details\")\n",
    "\n",
    "    print(\"\\nAI: \\\"I've applied sharpening to enhance the details. Here's your final image:\\\"\")\n",
    "    print(\"\\nOperations performed:\")\n",
    "    for func_call in metadata['functions_called']:\n",
    "        print(f\"- {func_call['name']}({', '.join([f'{k}={v}' for k, v in func_call['args'].items()])})\")\n",
    "\n",
    "    display_before_after(previous_image, current_image, [\"Fine-tuned\", \"Final Image\"])\n",
    "\n",
    "    # Show the complete transformation\n",
    "    print(\"\\nComplete Transformation:\")\n",
    "    display_before_after(image, current_image, [\"Original Image\", \"Final Edited Image\"])\n",
    "\n",
    "    return current_image\n",
    "\n",
    "# Run the conversational editing workflow\n",
    "final_image = conversational_editing_workflow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d60fafd3ecfaf",
   "metadata": {},
   "source": [
    "# Part 3: RAG-Based Style Recommendations\n",
    "\n",
    "Our photo editor also uses Retrieval Augmented Generation (RAG) to recommend cinematic styles based on image content. This combines a knowledge base of cinematography techniques with AI image analysis to suggest styles that match the content of the photo.\n",
    "\n",
    "## Understanding Retrieval Augmented Generation (RAG) for Style Recommendations\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a powerful AI technique that combines the strengths of retrieval-based systems with generative models. In our photo editing application, RAG works by:\n",
    "\n",
    "1. **Retrieving relevant information** from a knowledge base of cinematography styles and techniques\n",
    "2. **Augmenting the recommendation process** with this retrieved knowledge\n",
    "3. **Generating style recommendations** that are tailored to the specific image content\n",
    "\n",
    "This approach has several advantages over traditional preset filters:\n",
    "\n",
    "- **Content-aware recommendations**: Styles are suggested based on what's in the photo\n",
    "- **Educational value**: Users learn about cinematography techniques and why they work\n",
    "- **Flexibility**: The system can adapt to both image content and user descriptions\n",
    "- **Transparency**: Clear explanations for why each style is recommended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66216be08d0a5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG style engine\n",
    "rag_engine = RAGStyleEngine()\n",
    "\n",
    "# Get style recommendations based on image content\n",
    "recommendations = rag_engine.recommend_style(image)\n",
    "\n",
    "# Display the recommendations\n",
    "print(\"Style Recommendations Based on Image Content:\")\n",
    "for i, rec in enumerate(recommendations):\n",
    "    print(f\"\\n{i+1}. {rec['style']} (Score: {rec['score']})\")\n",
    "    print(f\"   Description: {rec['description']}\")\n",
    "    print(f\"   Reasoning:\")\n",
    "    for reason in rec['reasoning']:\n",
    "        print(f\"   - {reason}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440fc1a3cc5a3c2f",
   "metadata": {},
   "source": [
    "Now, let's apply these recommended styles to our image and see the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f868426a44d2447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the recommended styles\n",
    "styled_images = []\n",
    "style_names = []\n",
    "\n",
    "for rec in recommendations:\n",
    "    style_name = rec['style']\n",
    "    styled = rag_engine.apply_style(image, style_name)\n",
    "    styled_images.append(styled)\n",
    "    style_names.append(style_name)\n",
    "\n",
    "# Display the original and styled images\n",
    "display_multiple([image] + styled_images, [\"Original\"] + style_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d6bf4620eda5a",
   "metadata": {},
   "source": [
    "## Style Recommendations Based on Description\n",
    "\n",
    "We can also recommend styles based on a description provided by the user. This allows users to describe the look they want in natural language, and the system will find matching styles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b59c68fa5d3adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get style recommendations based on a description\n",
    "description = \"I want a dramatic movie look with high contrast\"\n",
    "desc_recommendations = rag_engine.recommend_style(image, description)\n",
    "\n",
    "# Display the recommendations\n",
    "print(f\"Style Recommendations Based on Description: '{description}'\")\n",
    "for i, rec in enumerate(desc_recommendations):\n",
    "    print(f\"\\n{i+1}. {rec['style']} (Score: {rec['score']})\")\n",
    "    print(f\"   Description: {rec['description']}\")\n",
    "    print(f\"   Reasoning:\")\n",
    "    for reason in rec['reasoning']:\n",
    "        print(f\"   - {reason}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ce10f2b303057",
   "metadata": {},
   "source": [
    "Let's apply these description-based style recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd1807bf53592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the description-based recommended styles\n",
    "desc_styled_images = []\n",
    "desc_style_names = []\n",
    "\n",
    "for rec in desc_recommendations:\n",
    "    style_name = rec['style']\n",
    "    styled = rag_engine.apply_style(image, style_name)\n",
    "    desc_styled_images.append(styled)\n",
    "    desc_style_names.append(style_name)\n",
    "\n",
    "# Display the original and styled images\n",
    "display_multiple([image] + desc_styled_images, [\"Original\"] + desc_style_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736648b58d9b3fa",
   "metadata": {},
   "source": [
    "## Style-Based Storytelling\n",
    "\n",
    "One of the most powerful applications of RAG-based style recommendations is helping users tell visual stories through consistent styling. Different scenes in a story might require different cinematic looks to convey the right mood and atmosphere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d45a86f0970569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_based_storytelling():\n",
    "    \"\"\"Demonstrate style-based storytelling with RAG recommendations.\"\"\"\n",
    "    print(\"=== Style-Based Storytelling ===\\n\")\n",
    "    print(\"Imagine you're creating a visual story with different scenes. Each scene needs a different mood and atmosphere.\")\n",
    "\n",
    "    # Define our story scenes\n",
    "    story_scenes = [\n",
    "        {\n",
    "            \"scene_name\": \"Opening Scene - Mysterious Beginning\",\n",
    "            \"description\": \"The story begins with a mysterious, moody atmosphere\",\n",
    "            \"style_query\": \"mysterious dark moody atmosphere like a thriller\"\n",
    "        },\n",
    "        {\n",
    "            \"scene_name\": \"Flashback Scene - Nostalgic Past\",\n",
    "            \"description\": \"We flash back to happier times in the past\",\n",
    "            \"style_query\": \"warm nostalgic vintage look\"\n",
    "        },\n",
    "        {\n",
    "            \"scene_name\": \"Action Scene - Dramatic Confrontation\",\n",
    "            \"description\": \"The protagonist faces a dramatic confrontation\",\n",
    "            \"style_query\": \"dramatic high contrast action movie style\"\n",
    "        },\n",
    "        {\n",
    "            \"scene_name\": \"Resolution Scene - Hopeful Ending\",\n",
    "            \"description\": \"The story resolves with a hopeful, uplifting ending\",\n",
    "            \"style_query\": \"bright vibrant hopeful cinematic\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Process each scene\n",
    "    for scene in story_scenes:\n",
    "        print(f\"\\n{'='*80}\\n{scene['scene_name']}\\n{'='*80}\")\n",
    "        print(f\"Scene Description: {scene['description']}\")\n",
    "        print(f\"Style Query: \\\"{scene['style_query']}\\\"\")\n",
    "\n",
    "        # Get style recommendations for this scene\n",
    "        recommendations = rag_engine.recommend_style(image, scene['style_query'])\n",
    "\n",
    "        # Display the top recommendation\n",
    "        if recommendations:\n",
    "            top_rec = recommendations[0]\n",
    "            print(f\"\\nRecommended Style: {top_rec['style']} (Score: {top_rec['score']})\")\n",
    "            print(f\"Style Description: {top_rec['description']}\")\n",
    "            print(\"Reasoning:\")\n",
    "            for reason in top_rec['reasoning']:\n",
    "                print(f\"- {reason}\")\n",
    "\n",
    "            # Apply the style\n",
    "            styled_image = rag_engine.apply_style(image, top_rec['style'])\n",
    "\n",
    "            # Display the result\n",
    "            display_before_after(image, styled_image, \n",
    "                               [\"Original Image\", f\"{scene['scene_name']} with '{top_rec['style']}' style\"])\n",
    "\n",
    "# Run the storytelling demonstration\n",
    "style_based_storytelling()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe1b29592424bb1",
   "metadata": {},
   "source": [
    "# Part 4: Innovative Use Case - AI-Guided Creative Photography\n",
    "\n",
    "Now let's explore an innovative use case that combines all of these AI capabilities: AI-guided creative photography. In this scenario, the AI analyzes an image, suggests creative directions, and helps the user achieve a specific artistic vision.\n",
    "\n",
    "This approach is particularly valuable for:\n",
    "- Amateur photographers looking to achieve professional results\n",
    "- Creative professionals seeking inspiration\n",
    "- Educators teaching photography and editing techniques\n",
    "\n",
    "Let's see how this works in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308c807efdfc00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_guided_creative_workflow(image, creative_direction):\n",
    "    \"\"\"Demonstrate an AI-guided creative workflow.\n",
    "\n",
    "    Args:\n",
    "        image: Input image\n",
    "        creative_direction: Description of the desired creative direction\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (final image, workflow steps)\n",
    "    \"\"\"\n",
    "    workflow_steps = []\n",
    "    results = [image.copy()]\n",
    "\n",
    "    # Step 1: Analyze the image\n",
    "    ai_analyzer = AIImageAnalyzer()\n",
    "    adjustments, analysis = ai_analyzer.analyze(image)\n",
    "\n",
    "    workflow_steps.append({\n",
    "        \"step\": \"Image Analysis\",\n",
    "        \"description\": f\"AI analyzed the image and identified it as a {analysis['scene_type']} scene with {analysis['lighting_condition']} lighting.\"\n",
    "    })\n",
    "\n",
    "    # Step 2: Apply basic adjustments\n",
    "    basic_adjusted = apply_adjustments(image, adjustments)\n",
    "    results.append(basic_adjusted)\n",
    "\n",
    "    workflow_steps.append({\n",
    "        \"step\": \"Basic Adjustments\",\n",
    "        \"description\": f\"Applied {len(adjustments)} AI-recommended adjustments to optimize the image.\"\n",
    "    })\n",
    "\n",
    "    # Step 3: Find styles matching the creative direction\n",
    "    rag_engine = RAGStyleEngine()\n",
    "    style_recs = rag_engine.recommend_style(image, creative_direction)\n",
    "\n",
    "    if style_recs:\n",
    "        # Apply the top recommended style\n",
    "        top_style = style_recs[0]['style']\n",
    "        styled_image = rag_engine.apply_style(basic_adjusted, top_style)\n",
    "        results.append(styled_image)\n",
    "\n",
    "        workflow_steps.append({\n",
    "            \"step\": \"Style Application\",\n",
    "            \"description\": f\"Applied '{top_style}' style based on the creative direction: '{creative_direction}'\"\n",
    "        })\n",
    "\n",
    "    # Step 4: Fine-tune with natural language processing\n",
    "    nl_processor = NLProcessor()\n",
    "\n",
    "    # Register the same functions as before\n",
    "    nl_processor.register_function(\"adjust_exposure\", adjust_exposure, \n",
    "                                 \"Adjust the brightness/exposure of the image\",\n",
    "                                 {\"amount\": {\"type\": \"number\", \"description\": \"Amount to adjust exposure (-1.0 to 1.0)\"}})\n",
    "\n",
    "    nl_processor.register_function(\"adjust_contrast\", adjust_contrast,\n",
    "                                 \"Adjust the contrast of the image\",\n",
    "                                 {\"multiplier\": {\"type\": \"number\", \"description\": \"Contrast multiplier (0.5 to 2.0)\"}})\n",
    "\n",
    "    nl_processor.register_function(\"adjust_saturation\", adjust_saturation,\n",
    "                                 \"Adjust the color saturation of the image\",\n",
    "                                 {\"adjustment\": {\"type\": \"number\", \"description\": \"Saturation adjustment (-1.0 to 1.0)\"}})\n",
    "\n",
    "    nl_processor.register_function(\"adjust_temperature\", adjust_temperature,\n",
    "                                 \"Adjust the color temperature (warmth/coolness) of the image\",\n",
    "                                 {\"adjustment\": {\"type\": \"number\", \"description\": \"Temperature adjustment (-0.5 to 0.5)\"}})\n",
    "\n",
    "    nl_processor.register_function(\"adjust_sharpness\", adjust_sharpness,\n",
    "                                 \"Adjust the sharpness/clarity of the image\",\n",
    "                                 {\"strength\": {\"type\": \"number\", \"description\": \"Sharpness strength (0.0 to 1.0)\"}})\n",
    "\n",
    "    nl_processor.register_function(\"reduce_noise\", reduce_noise,\n",
    "                                 \"Reduce noise/grain in the image\",\n",
    "                                 {\"strength\": {\"type\": \"number\", \"description\": \"Noise reduction strength (0.0 to 1.0)\"}})\n",
    "\n",
    "    # Generate a fine-tuning instruction based on the creative direction\n",
    "    fine_tuning_instruction = f\"Fine-tune for {creative_direction}\"\n",
    "    final_image, metadata = nl_processor.process(results[-1], fine_tuning_instruction)\n",
    "    results.append(final_image)\n",
    "\n",
    "    workflow_steps.append({\n",
    "        \"step\": \"Fine-tuning\",\n",
    "        \"description\": f\"Applied natural language fine-tuning: '{fine_tuning_instruction}'\",\n",
    "        \"functions\": [f\"{func['name']}({', '.join([f'{k}={v}' for k, v in func['args'].items()])})\" \n",
    "                     for func in metadata['functions_called']]\n",
    "    })\n",
    "\n",
    "    return final_image, results, workflow_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e016dfa2615bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the AI-guided creative workflow with different creative directions\n",
    "creative_directions = [\n",
    "    \"dramatic cinematic look\",\n",
    "    \"warm vintage feel\",\n",
    "    \"professional portrait style\"\n",
    "]\n",
    "\n",
    "for direction in creative_directions:\n",
    "    print(f\"\\n\\n=== AI-Guided Creative Workflow: '{direction}' ===\\n\")\n",
    "\n",
    "    final_image, workflow_images, steps = ai_guided_creative_workflow(image, direction)\n",
    "\n",
    "    # Display the workflow steps\n",
    "    for i, step in enumerate(steps):\n",
    "        print(f\"\\nStep {i+1}: {step['step']}\")\n",
    "        print(f\"  {step['description']}\")\n",
    "        if 'functions' in step:\n",
    "            print(\"  Functions applied:\")\n",
    "            for func in step['functions']:\n",
    "                print(f\"  - {func}\")\n",
    "\n",
    "    # Display the workflow images\n",
    "    step_titles = [\"Original\"] + [step[\"step\"] for step in steps]\n",
    "    display_multiple(workflow_images, step_titles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59818d328aeefaf4",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this comprehensive notebook, we've demonstrated how generative AI transforms the photo editing experience by making it more accessible, intelligent, and efficient. We've explored three key innovations:\n",
    "\n",
    "1. **AI-powered image analysis** that understands content and recommends appropriate adjustments\n",
    "2. **Natural language photo editing** that allows users to describe what they want in plain English\n",
    "3. **RAG-based style recommendations** that suggest cinematic styles based on image content and user descriptions\n",
    "\n",
    "We've also shown how these capabilities can be combined in an **AI-guided creative workflow** that helps users achieve specific artistic visions.\n",
    "\n",
    "These AI-powered approaches democratize photo editing by removing the technical barriers that traditionally made it difficult for casual photographers to achieve professional-looking results. By understanding what's in the photo and what the user wants to achieve, the AI can guide them through the editing process and help them create images that match their creative vision.\n",
    "\n",
    "Key benefits include:\n",
    "\n",
    "- **Accessibility**: No technical knowledge required to achieve professional results\n",
    "- **Efficiency**: Faster editing with fewer steps and trial-and-error\n",
    "- **Intuitiveness**: Edit using familiar language instead of technical controls\n",
    "- **Education**: Learn about cinematography techniques and visual aesthetics\n",
    "- **Personalization**: Get recommendations tailored to specific image content\n",
    "\n",
    "The future of photo editing is not about replacing human creativity, but about augmenting it with AI that understands both the technical aspects of photography and the artistic intentions of the user.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
